{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Thymio-project-introduction\" data-toc-modified-id=\"Thymio-project-introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Thymio project introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Thymio-preparation\" data-toc-modified-id=\"Thymio-preparation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Thymio preparation</a></span></li></ul></li><li><span><a href=\"#Complete-Program\" data-toc-modified-id=\"Complete-Program-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Complete Program</a></span><ul class=\"toc-item\"><li><span><a href=\"#Main-program-description\" data-toc-modified-id=\"Main-program-description-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Main program description</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization-and-decision-making\" data-toc-modified-id=\"Initialization-and-decision-making-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Initialization and decision making</a></span></li><li><span><a href=\"#Data-acquisition\" data-toc-modified-id=\"Data-acquisition-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Data acquisition</a></span></li><li><span><a href=\"#Control\" data-toc-modified-id=\"Control-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Control</a></span></li></ul></li><li><span><a href=\"#Complete-program-execution\" data-toc-modified-id=\"Complete-program-execution-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Complete program execution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Define-the-state-machine\" data-toc-modified-id=\"Define-the-state-machine-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Define the state machine</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-the-data-structures\" data-toc-modified-id=\"Define-the-data-structures-2.2.2.1\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>Define the data structures</a></span></li><li><span><a href=\"#Define-the-state-functions-and-the-state-machine-itself\" data-toc-modified-id=\"Define-the-state-functions-and-the-state-machine-itself-2.2.2.2\"><span class=\"toc-item-num\">2.2.2.2&nbsp;&nbsp;</span>Define the state functions and the state machine itself</a></span></li></ul></li><li><span><a href=\"#Program-execution\" data-toc-modified-id=\"Program-execution-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Program execution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adjusting-camera-position\" data-toc-modified-id=\"Adjusting-camera-position-2.2.3.1\"><span class=\"toc-item-num\">2.2.3.1&nbsp;&nbsp;</span>Adjusting camera position</a></span></li><li><span><a href=\"#Execute-Program\" data-toc-modified-id=\"Execute-Program-2.2.3.2\"><span class=\"toc-item-num\">2.2.3.2&nbsp;&nbsp;</span>Execute Program</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vision</a></span><ul class=\"toc-item\"><li><span><a href=\"#Corner-detection\" data-toc-modified-id=\"Corner-detection-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Corner detection</a></span></li><li><span><a href=\"#Image-Straightening\" data-toc-modified-id=\"Image-Straightening-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Image Straightening</a></span></li><li><span><a href=\"#Obstacles-Processing\" data-toc-modified-id=\"Obstacles-Processing-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Obstacles Processing</a></span></li><li><span><a href=\"#Location-of-endpoint-and-of-Thymio\" data-toc-modified-id=\"Location-of-endpoint-and-of-Thymio-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Location of endpoint and of Thymio</a></span></li></ul></li><li><span><a href=\"#Path-planning\" data-toc-modified-id=\"Path-planning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Path planning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-path-planner\" data-toc-modified-id=\"Initialise-path-planner-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initialise path planner</a></span></li><li><span><a href=\"#Planning-phase\" data-toc-modified-id=\"Planning-phase-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Planning phase</a></span></li><li><span><a href=\"#Query-Phase\" data-toc-modified-id=\"Query-Phase-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Query Phase</a></span></li><li><span><a href=\"#Getting-the-path\" data-toc-modified-id=\"Getting-the-path-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Getting the path</a></span></li><li><span><a href=\"#complete-pathplanning-demonstration\" data-toc-modified-id=\"complete-pathplanning-demonstration-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>complete pathplanning demonstration</a></span></li></ul></li><li><span><a href=\"#Local-navigation,-avoidance-of-unexpected-obstacles\" data-toc-modified-id=\"Local-navigation,-avoidance-of-unexpected-obstacles-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Local navigation, avoidance of unexpected obstacles</a></span></li><li><span><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Removal-of-unstable-camera-data\" data-toc-modified-id=\"Removal-of-unstable-camera-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Removal of unstable camera data</a></span></li></ul></li><li><span><a href=\"#Robot-control\" data-toc-modified-id=\"Robot-control-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Robot control</a></span><ul class=\"toc-item\"><li><span><a href=\"#Odometry\" data-toc-modified-id=\"Odometry-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Odometry</a></span></li><li><span><a href=\"#path-following\" data-toc-modified-id=\"path-following-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>path following</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name of Group Members\n",
    "The group members that developed this project are:\n",
    "\n",
    "Balestrini, Théophile \n",
    "\n",
    "Bou Akl, Carl\n",
    "\n",
    "Michel, Tobias Marcel\n",
    "\n",
    "Ovide Sanchez, Elio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thymio project introduction\n",
    "The goal of this project is to make the thymio navigate a map using all information available. This means the thymio needs to track its position using odometry based on the velocity reading of the wheels, use his infrared distance sensors to detect obstacles and avoid them, and use a camera from where a map should be created on which navigation is done.\n",
    "To fulfill these rather broad specifications, we defined an environment, in which this will happen. \n",
    "The map is an A0-paper of dimensions 118.9 cm by 84.1 cm. Close to the corner of this paper should be red corner makers, to help the vision correct small misalignments of the camera. The obstacles need to be black and preferably figures with 90° angles. These also should be made from paper and not posses to great a height. The goal where the thymio needs to travel to is then marked by a green star. The camera should be looking onto the paper from above the A0 paper. As close to vertical as possible.\n",
    "The lighting used to test is also not a really bright light, and the exposure of the camera is set accordingly. The lighting should be as uniform as possible, but it should not be really bright. The thymio itself should be marked with 2 blue dots. There is also more preparation needed, which is described in chapter 1.1.\n",
    "\n",
    "The image below demonstrates an example\n",
    "\n",
    "<img src=\"documentation\\ImagesForDocumentation/exampleMap.jpg\"\n",
    "     alt=\"Example Picture of camera view\"\n",
    "     style=\"float: left; margin-right: 200px;width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is an image of the template map. This is also provided as A0-pdf in the documentation folder\n",
    "\n",
    "<img src=\"documentation\\ImagesForDocumentation/mapImage.jpg\"\n",
    "     alt=\"Example Picture of camera view\"\n",
    "     style=\"float: left; margin-right: 5px;width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thymio preparation\n",
    "The thymio should have 2 blue dots on it. 1 big, where the wheels are and 1 smaller on the front part. Those dots should be on a light blocking paper which is then mounted on the thymio. Furthermore there are some lights that can't be turned off. These should also be covered as well as possible.\n",
    "\n",
    "\n",
    "<img src=\"documentation/ImagesForDocumentation/thymioTop.jpg\"\n",
    "     alt=\"Thymio Preparation 1\"\n",
    "     style=\"float: left; margin-right: 10px;width: 300px;\" />\n",
    "\n",
    "<img src=\"documentation/ImagesForDocumentation/thymioBottom.jpg\"\n",
    "     alt=\"Thymio Preparation 2\"\n",
    "     style=\"float: left; margin-right: 10px;width: 200px;\" />\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"documentation/ImagesForDocumentation/thymioBehind.jpg\"\n",
    "     alt=\"Thymio Preparation 3\"\n",
    "     style=\"float: left; margin-right: 10px;width: 300px;\" />\n",
    "Left most Image shows the top and side cover points\n",
    "Middle Image shows the bottom spots to cover\n",
    "RIght most Image shows the behind spots to cover\n",
    "\n",
    "If all these spots are covered it wont confuse the vision system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Program\n",
    "In this section  the main execution of the program is presented and available to be executed with a thymio that is properly prepared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program description\n",
    "The main program consists out of four parts. The first one is the initialization, in this part the different Connections are made, like the one to the camera and the thymio. It also generates the map and the path.The second Part Data acquisition takes data  from the different senors present. These are the camera, the horizontal facing infrared distance sensors and the internal speed measurement. These are then processed into the position and orientation of the robot and preprocessed to be able to make decisions in the second part. The second part is the state machine that takes the preprocessed data from the sensor part and decides whether it should follow a the path or if it should avoid path, and so on. The fourth part takes the decisions of the the second part and controls the robot with them and the data of the first part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and decision making\n",
    "<img src=\"documentation/stateEventRobotics.jpg\"\n",
    "     alt=\"Robotics statemachine\"\n",
    "     style=\"float: left; margin-right: 5px;width: 700px;\" />\n",
    "To initialize everything properly, first the connection to the camera is established, when this fails everything else will also not work and it is necessary to setup the camera correctly. To facilitate this at the beginning the images captured by the camera are shown at the beginning. Then the connection to the thymio is made, also a very important component. If the connection was successful too, then everything can proceed to generate the map. For that the camera is used to capture the map and generate an occupancy grid. Then the path is planned by the path planner from the current thymio location to the goal. After that the main loop begins which is divided into 3 parts. The decision making part is manly the implementation of the state machine shown to the left without the Initialization part.\n",
    "This means that the robot follows the path as long as it has not reached the goal or encounters a obstacle. If an obstacle is encountered the Obstacle avoidance is triggered and it is avoided until no obstacle is detected anymore. Then the robot proceeds to the same way point that he tried to steer to before. This happens until the goal is reached. If the thymio reaches the goal it stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data acquisition\n",
    "The data from the different sensors is first processed before it is send into the control and the decision making part.\n",
    "This means the position data from the camera is filtered with a kalman filter in combination with the odometry from the speed readings. This only happens when the position estimate from the camera is not unstable and jumping around. When this happens the data from the camera is ignored until it has stabilized. If the position estimate from the camera has stabilized again, it is then used again. This is necessary because sometimes the position reading from the camera jumps around. This in theory also enables the detection of kidnapping, but that is not implemented. Beside checking if the position given by the camera is stable, it is also check whether or not the thymio can be see by the camera. If this is not the case the robot relies on the odometry alone. Beside the data from the camera and the odometry the horizontal infrared distance sensors are also checks whether or not they see an obstacle. If they do, a flag is set to tell the state machine this fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control\n",
    "In the control section of the main loop the path follower follows the path generated by the path planner. This path consists out of way points the robot has to reach. For that the path follower compares the current position of the thymio with the next way point, not the closest but the next in the list, even though they are save in an array, and then drives the robot towards it. If the goal is reached the robot then stops. If the decision making state machine says to stop the robot also stops. One would think that the obstacle avoidance is also located here. That is not the case becuase of the way it is coded. The obstacle avoidance is a function that stays inside itself until the obstacle is avoided. This means the obstacle avoidance is performed inside the state machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete program execution\n",
    "In this section one can execute the different cells one after the other to get the behavior described before. If they are not all executed in order then it wont work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "All the imports necessary to run the program. This includes user defined files and library files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:52:42.121860Z",
     "start_time": "2020-08-29T12:52:42.112779Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pyserial\n",
    "#import standart libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import serial\n",
    "import numpy as np\n",
    "from numpy import linalg as LNG \n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the script\n",
    "# with the Thymio class and all the files with the group generated functions and classes\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from Thymio import Thymio\n",
    "#import functions made by group\n",
    "from pathPlanning import pathPlaning\n",
    "import ANN\n",
    "import robot_control\n",
    "import Vision\n",
    "import sys\n",
    "\n",
    "from kalman_filter import kalman_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the state machine\n",
    "In this section the state machine shown earlier is realized in code. To achieve that, 2 data structures are needed, one that holds the names of the states and one that holds all the data needed to make the decisions and afterwards the state functions are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the data structures\n",
    "The first data structure holds the state names used in the state machine. This is basically what in other languages is an enumeration or static variables. The second class is used to store data from the sensors and then only needing to hand over the object of this class, containing all the data needed to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class used to store the names of the states to be able to retrive the state functions from the dictionary \n",
    "#that serves as state machine\n",
    "class stateNames_t:\n",
    "    def __init__(self):\n",
    "        self.planAcquired='planAcquired'\n",
    "        self.newPath='newpath'\n",
    "        self.checkingPath='checkingPath'\n",
    "        self.underWay='underWay'\n",
    "        self.obstacleAvoidance='obstalceAvoidance'\n",
    "        self.goalReached='goalReached'\n",
    "#this class is used as data transfer between the data acqusition part, the decision making part and the control part.\n",
    "class FSMHelper:\n",
    "    def __init__(self,thymio,equalTolerance,wayPointDistance,pathplanner):\n",
    "        self.wasKidnapped=False\n",
    "        self.tolerance=equalTolerance\n",
    "        self.currentPosition=np.zeros((2,1))\n",
    "        self.pathPlanner=pathplanner\n",
    "        self.newPositionEstimate=0    \n",
    "        self.thymio=thymio      \n",
    "        self.obstacleDetected=False\n",
    "        self.doStop=False\n",
    "        self.goalReached=False\n",
    "        self.goal=np.zeros((2,1))\n",
    "        self.pathToFollow=np.array([[0],[0]])\n",
    "        self.FSMStates=stateNames_t()\n",
    "        self.straightenedImage=0\n",
    "        self.wayPointReachedDistance=wayPointDistance\n",
    "        self.followPath=False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the state functions and the state machine itself\n",
    "In this sections all the state functions are defined. Each function is called when the state it represents is entered. Each state function then returns the name of the next state, that should be entered. This makes it possible that the state machine is a dictionary with the state names as keys and the state functions as items. To make the state machine to its job, the item with the current state name has to be retrieved, the state function, and executed. The name given by the executed state function is then the next state and the same can be done again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planAcquired(robot):     #starting state, the map is generated and preprocessed to enable path generation\n",
    "    return robot.FSMStates.checkingPath      #go to check if a new path needs to be generated\n",
    "    \n",
    "def newPath(robot):#\n",
    "    robot.pathToFollow=robot.pathPlanner.getOptimizedPath()\n",
    "    print(\"path after Planner\",robot.pathToFollow)\n",
    "    return robot.FSMStates.checkingPath\n",
    "    \n",
    "def checkingPath(robot):\n",
    "    if(np.size(robot.pathToFollow)!=0):\n",
    "        robot.followPath=True\n",
    "        return robot.FSMStates.underWay\n",
    "    else:\n",
    "        robot.followPath=False\n",
    "        robot.doStop=True\n",
    "        return robot.FSMStates.goalReached\n",
    "    \n",
    "def underWay(robot):\n",
    "    if(robot.obstacleDetected==True):\n",
    "        robot.followPath=False\n",
    "        return robot.FSMStates.obstacleAvoidance\n",
    "    else:\n",
    "        return robot.FSMStates.planAcquired\n",
    "    \n",
    "def avoidObstacle(robot):     \n",
    "    ANN.run_ann_without_memory(robot.thymio)\n",
    "    return robot.FSMStates.checkingPath \n",
    "\n",
    "def goalReached(robot):\n",
    "    robot.goalReached=True\n",
    "    \n",
    "    \n",
    "#define the concrete stateName object to make the dictionary for the actual state machine\n",
    "stateName=stateNames_t()\n",
    "switch = {\n",
    "    stateName.planAcquired     : planAcquired,\n",
    "    stateName.newPath          : newPath,\n",
    "    stateName.checkingPath     : checkingPath,\n",
    "    stateName.underWay         : underWay,\n",
    "    stateName.obstacleAvoidance: avoidObstacle,\n",
    "    stateName.goalReached      : goalReached,\n",
    "}\n",
    "currentState=stateName.planAcquired\n",
    "futureState=stateName.planAcquired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data structures needed to run the state machine and the state machine itself is defined in code, the program can be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting camera position\n",
    "<img src=\"documentation\\ImagesForDocumentation/exampleMap.jpg\"\n",
    "     alt=\"Example Picture of camera view\"\n",
    "     style=\"float: left; margin-right: 5px;width: 500px;\" />\n",
    "The camera should see an image similar to the one shown here. All 4 red corner markers should be visible, the map should also fill out most of the image of the camera. It is also necessary that the colors are well visible and that the picture is not too bright. To help with this adjustment execute the code in the next cell. The exposure setting of the camera can be adjusted, with the presses of \"w\" and \"s\". Just don't press to fast. The values are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameraIndex=1             #index 1 is the index of the specific machine used. it is highly likly that it is on other machines aswell\n",
    "cv2.namedWindow(\"preview\")\n",
    "videoCapture = cv2.VideoCapture(cameraIndex)\n",
    "if not(videoCapture.isOpened()):\n",
    "    raise Exception('could not connect to camera')\n",
    "videoCapture = cv2.VideoCapture(cameraIndex)\n",
    "exposureOfCamera=-2.1          #initial exposure\n",
    "videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera)\n",
    "videoCapture.set(cv2.CAP_PROP_SATURATION ,120)\n",
    "\n",
    "if not(videoCapture.isOpened()):\n",
    "    raise Exception('could not connect to camera')\n",
    "\n",
    "if videoCapture.isOpened(): # try to get the first frame\n",
    "    rval, frame = videoCapture.read()\n",
    "else:\n",
    "    rval = False\n",
    "    raise Exception('could not connect to camera')\n",
    "while rval:\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "    rval, frame = videoCapture.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "    elif(key== ord('w')):\n",
    "        exposureOfCamera=exposureOfCamera+0.1\n",
    "        print (videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera),exposureOfCamera)\n",
    "    elif(key== ord('s')):    \n",
    "        exposureOfCamera=exposureOfCamera-0.1\n",
    "        print (videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera),exposureOfCamera)\n",
    "cv2.destroyWindow(\"preview\")\n",
    "cv2.destroyAllWindows()\n",
    "cv2.VideoCapture(cameraIndex).release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Program\n",
    "In this section the main control loop and the initialization can be executed. Connecting to the thymio should only be executed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "th=Thymio.serial(port=\"COM6\", refreshing_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the next cell is executed the thymio will begin moving after a certain amount of time. This waiting time is because of the map generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Coordinates:  (105.01018483440366, 10.095675691855687)\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "---------------------------------------------------------WAYPOINT REACHED\n",
      "GOAL REACHED\n"
     ]
    }
   ],
   "source": [
    "#########if exposure was adjusted fill in the value printted into the console into this variable instead of the -2.1\n",
    "exposureOfCamera=-2.1\n",
    "#------- turn off all leds that can be controlled\n",
    "th.set_var_array(\"leds.top\", [0, 0, 0])\n",
    "th.set_var_array(\"leds.bottom.right\", [0, 0, 0])\n",
    "th.set_var_array(\"leds.bottom.left\", [0, 0, 0])\n",
    "demonstrationSize=4        #how many times the size of 118 by 84 the picture should be presented\n",
    "\n",
    "#------ start camera\n",
    "cameraIndex=1             #index 1 is the index of the specific machine used. it is highly likly that it is on other machines aswell\n",
    "videoCapture = cv2.VideoCapture(cameraIndex)\n",
    "#-------set exposure to have a bright enough picture\n",
    "videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera)\n",
    "if videoCapture.isOpened(): # try to get the first frame\n",
    "    rval, frame = videoCapture.read()\n",
    "else:\n",
    "    rval = False\n",
    "    raise Exception('could not read image!')\n",
    "\n",
    "mask= cv2.imread('Images/cornerMask.JPG')           #read corner mask\n",
    "if mask.size==0:\n",
    "    raise Exception('Could not open Mask')\n",
    "    \n",
    "dimension_paper = [118.9,84.1] #cm A0\n",
    "dim = (int(dimension_paper[1]),int(dimension_paper[0]))\n",
    "# Switching red and blue channels\n",
    "frame[:, :, [0, 2]] = frame[:, :, [2, 0]]\n",
    "mask[:, :, [0, 2]] = mask[:, :, [2, 0]]\n",
    "\n",
    "#preprocess image data to facilitate map generation and thymio position accusition\n",
    "p2_1, p98_1 = np.percentile(frame, (2, 98))\n",
    "img_res1 = exposure.rescale_intensity(frame, in_range=(p2_1,p98_1))\n",
    "img1_gray = cv2.cvtColor(img_res1, cv2.COLOR_BGR2GRAY)\n",
    "threshold_bg=130\n",
    "#------generate map from camera\n",
    "percent = 0.9\n",
    "while True:\n",
    "        output = Vision.bg_clustering(img1_gray, (50,50),threshold_bg)\n",
    "        corner_location = Vision.corner_detection(output,mask) # Get the location of the 4 corners\n",
    "        img_straighten, M = Vision.four_point_transform(frame, corner_location) # Get the transformation matrix and the straighten img\n",
    "        if(img_straighten.shape[0] > output.shape[0]*percent and img_straighten.shape[1] > output.shape[1]*percent):\n",
    "            break\n",
    "        else:\n",
    "            rval, frame = videoCapture.read()\n",
    "            frame[:, :, [0, 2]] = frame[:, :, [2, 0]]\n",
    "            \n",
    "\n",
    "im_dim = img_straighten.shape\n",
    "obstacles = Vision.get_obstacles(img_straighten) \n",
    "thymio_coord = Vision.get_thymio_info(frame,M,dim,im_dim) \n",
    "endpoint_coord = Vision.get_endpoint_info(frame,M,dim,im_dim)\n",
    "low_res_img = cv2.resize(img_straighten, dsize=((dim[1], dim[0])))\n",
    "\n",
    "#---------make occupancy grid from map\n",
    "occupancyGrid=cv2.resize(obstacles, dsize=((dim[1], dim[0])))\n",
    "occupancyGrid=occupancyGrid>200\n",
    "plt.imshow(occupancyGrid)\n",
    "plt.show()\n",
    "occupancyGrid=occupancyGrid.astype(float)\n",
    "kernel2 = np.ones((7,7), np.uint8)\n",
    "occupancyGrid = cv2.dilate(occupancyGrid, kernel2, iterations=2)\n",
    "\n",
    "pathPlanner=pathPlaning(occupancyGrid.copy(),1,1)\n",
    "\n",
    "#give over the thymio connection and the tolerance for equality to 1e-6, the distance when it is considered way point reached\n",
    "#and the path planner to use\n",
    "robotStatus=FSMHelper(th,1e-6,0.5,pathPlanner)\n",
    "robot_control.th=th      #give pathfollower the connection to the thymio\n",
    "#---------start planning\n",
    "robotStatus.pathPlanner.setGoal(np.asarray(endpoint_coord))\n",
    "robotStatus.currentPosition=np.array([(thymio_coord[0][0]),(thymio_coord[0][1]),(thymio_coord[1])/180*np.pi])\n",
    "robotStatus.pathPlanner.setStart(robotStatus.currentPosition[0:2])\n",
    "robotStatus.pathToFollow=robotStatus.pathPlanner.getOptimizedPath()\n",
    "estimatedRobotPose=robotStatus.currentPosition\n",
    "\n",
    "#show path on image\n",
    "originalPath=robotStatus.pathToFollow.copy() \n",
    "plt.imshow(low_res_img)\n",
    "plt.scatter(thymio_coord[0][0],thymio_coord[0][1])\n",
    "plt.plot(originalPath[0],originalPath[1])\n",
    "plt.show()\n",
    "#-------------path planning and map generation complete\n",
    "%matplotlib qt\n",
    "\n",
    "#constant kalman matrixes\n",
    "A = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "C = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "Q = np.array([[0.1, 0, 0],[0, 0.1, 0],[0,0,0.1]]) # Dependent on the error (can be linked to stated as velocity, but velocity is not taken as state. )\n",
    "R = np.array([[0.1, 0, 0],[0, 0.1, 0],[0,0,0.1]])\n",
    "Q_ini = Q\n",
    "X = np.array([[estimatedRobotPose[0]],[estimatedRobotPose[1]],[estimatedRobotPose[2]]], dtype = \"float32\")\n",
    "L = []\n",
    "\n",
    "timeElapsed=np.array([0,0], dtype = 'float64')\n",
    "\n",
    "robotPositionUncertainty=np.zeros((3,3))         #variable to store the accuracy estimate from odmotry\n",
    "\n",
    "currentState=stateName.planAcquired              #initialize state event\n",
    "futureState=stateName.planAcquired\n",
    "originalPath=robotStatus.pathToFollow.copy()    #store path before loop, loop removes waypoint from non original path\n",
    "doPlotCounter=0                               #variable used to show the image taken of the camera after 10 loop executions\n",
    "deltaT=0.0                                   # variable used to store the time needed to execute the loop once\n",
    "tbefore=time.time()                          #deltaT helpers\n",
    "tnow=0                                       #deltaT helpers\n",
    "\n",
    "#lists to record data to be able to plot that later on\n",
    "unfilteredCameraCoordinates=list()\n",
    "unfilteredOdometrieCoordinates=list()\n",
    "afterKalmanCoordinates=list()\n",
    "\n",
    "cutOfDistance=5                 #distance in where successive camera measurments have to be to the last one to be considered stable\n",
    "cameraDataStableCounter=0       #counter to count how many camera measurments where stable in a row\n",
    "cameraEstimate=robotStatus.currentPosition    #used to store current camera position estimate\n",
    "newPositionEstimate=robotStatus.currentPosition\n",
    "cameraData=True             #flag used to store whether the camera can see the position of the thymio\n",
    "numberOfStableMeasurments=10    # minimum of measurments after one measument was unstable to be considered stable again\n",
    "while(True):         #main execution loop\n",
    "    ##---------------------------------------------read sensors  \n",
    "    \n",
    "    #----get robot position from camera\n",
    "    [frameCaptureSuccesfull,newPicture]=videoCapture.read()\n",
    "    if(frameCaptureSuccesfull==False):\n",
    "        raise Exception('could not read from camera')\n",
    "    newPicture[:, :, [0, 2]] = newPicture[:, :, [2, 0]]\n",
    "    thymio_coord = Vision.get_thymio_info(newPicture,M,dim,im_dim) # Do these online, and feed info to kalman filter\n",
    "    \n",
    "    if(thymio_coord[0][0]<0 or thymio_coord[0][1]<0 ):\n",
    "        cameraData=False\n",
    "    else:\n",
    "        cameraData=True\n",
    "    #------------------------------Removal of unstable camera data\n",
    "    \n",
    "    if(cameraData==True):       #if camera communicates it sees the thymio check if position is stable\n",
    "        newCameraEstimate=np.array([thymio_coord[0][0],thymio_coord[0][1],thymio_coord[1]/180.0*np.pi])\n",
    "        unfilteredCameraCoordinates.append((cameraEstimate[0],cameraEstimate[1]))\n",
    "        \n",
    "        if(LNG.norm(newCameraEstimate[0:2]-cameraEstimate[0:2])<cutOfDistance):\n",
    "            cameraDataStableCounter=cameraDataStableCounter+1\n",
    "                       \n",
    "        else:\n",
    "            cameraDataStableCounter=0\n",
    "        cameraEstimate=newCameraEstimate\n",
    "   \n",
    "    #---------get odometrie data\n",
    "    estimatedRobotPose,robotPositionUncertainty,timeElapsed=robot_control.odometry(robotStatus.currentPosition,robotPositionUncertainty,timeElapsed, robot_control.MAX_SPEED)\n",
    "    unfilteredOdometrieCoordinates.append((estimatedRobotPose[0],estimatedRobotPose[1]))\n",
    "    \n",
    "    \n",
    "    if(cameraData==True and (LNG.norm(estimatedRobotPose[0:2]-cameraEstimate[0:2])<cutOfDistance or cameraDataStableCounter>numberOfStableMeasurments)):\n",
    "        newPositionEstimate=cameraEstimate\n",
    "    else:\n",
    "        newPositionEstimate=estimatedRobotPose\n",
    "\n",
    "   \n",
    "    #------------filtering\n",
    "    V_left=th[\"motor.left.speed\"]*0.0135\n",
    "    V_right=th[\"motor.right.speed\"]*0.0135\n",
    "    v_avg = (V_left + V_right)/2\n",
    "    v_delta = V_right-V_left\n",
    "\n",
    "    tnow=time.time()\n",
    "    deltaT=tnow-tbefore\n",
    "    tbefore=tnow\n",
    "\n",
    "    #kalman matrixes\n",
    "    B = np.array([[(math.cos(newPositionEstimate[2])),0.], [(math.sin(newPositionEstimate[2])), 0.], [0.,math.atan(v_delta*deltaT/9.5)]])\n",
    "    Z = np.array([[newPositionEstimate[0]],[newPositionEstimate[1]],[newPositionEstimate[2]]], dtype = \"float32\")\n",
    "    u = np.array([[v_avg*deltaT],[1]])  \n",
    "    X=np.array([[robotStatus.currentPosition[0]],[robotStatus.currentPosition[1]],[robotStatus.currentPosition[2]]])\n",
    "    thymio_position,cov = kalman_filt(X,u,Q_ini,Z,A,B,C,Q,R)\n",
    "    Q_ini = cov\n",
    "    robotStatus.currentPosition=newPositionEstimate\n",
    "    afterKalmanCoordinates.append((robotStatus.currentPosition[0],robotStatus.currentPosition[1]))\n",
    "                                       \n",
    "                                       \n",
    "    #------------check if unexcpected obstacle is present\n",
    "    robotStatus.obstacleDetected=not(all(sensorValues==0 for sensorValues in robotStatus.thymio[\"prox.horizontal\"]))\n",
    "   \n",
    "    #------------------------------make desicions and work with the collected data \n",
    "    \n",
    "    \n",
    "    \n",
    "    stateToExecute=switch.get(currentState)\n",
    "\n",
    "    futureState=stateToExecute(robotStatus)\n",
    "    \n",
    "    #--------------------------------controll the robot\n",
    "    \n",
    "    #stopping robot if goal reached end programm\n",
    "    if(robotStatus.followPath==True and np.size(robotStatus.pathToFollow)!=0):\n",
    "        _,robotStatus.pathToFollow=robot_control.path_following(robotStatus.currentPosition,robotStatus.pathToFollow)\n",
    "    if(robotStatus.doStop==True):\n",
    "        robotStatus.thymio.set_var(\"motor.left.target\", 0)\n",
    "        robotStatus.thymio.set_var(\"motor.right.target\", 0)\n",
    "    if(robotStatus.goalReached==True):\n",
    "        robotStatus.thymio.set_var(\"motor.left.target\", 0)\n",
    "        robotStatus.thymio.set_var(\"motor.right.target\", 0)\n",
    "        break\n",
    "    currentState=futureState\n",
    "    doPlotCounter=doPlotCounter+1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------------------------------do live plotting\n",
    "    if(doPlotCounter>10 or robotStatus.goalReached==True):\n",
    "        \n",
    "        doPlotCounter=0\n",
    "        low_res_img = cv2.resize(newPicture, dsize=((int(dim[1]*demonstrationSize), int(dim[0]*demonstrationSize))))\n",
    "        low_res_img[:, :, [0, 2]] = low_res_img[:, :, [2, 0]]\n",
    "        low_res_img = cv2.circle(low_res_img, (int(robotStatus.currentPosition[0]*demonstrationSize),int(robotStatus.currentPosition[1]*demonstrationSize)), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        low_res_img = cv2.circle(low_res_img, (int(cameraEstimate[0]*demonstrationSize),int(cameraEstimate[1]*demonstrationSize)), radius=10, color=(0, 255, 0), thickness=2)\n",
    "        low_res_img = cv2.circle(low_res_img, (int(estimatedRobotPose[0]*demonstrationSize),int(estimatedRobotPose[1]*demonstrationSize)), radius=8, color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        cv2.imshow(\"Display window\",low_res_img)\n",
    "        cv2.waitKey(1)\n",
    "            \n",
    "\n",
    "#-----------------------------------plot collected data\n",
    "unfilteredOdometrieCoordinates=np.array(unfilteredOdometrieCoordinates).reshape(-1, 2).transpose()\n",
    "afterKalmanCoordinates=np.array(afterKalmanCoordinates).reshape(-1, 2).transpose()\n",
    "unfilteredCameraCoordinates=np.array(unfilteredCameraCoordinates).reshape(-1, 2).transpose()\n",
    "plt.scatter(unfilteredCameraCoordinates[0]*demonstrationSize,unfilteredCameraCoordinates[1]*demonstrationSize,label=\"unfiltered camera\")\n",
    "plt.scatter(unfilteredOdometrieCoordinates[0]*demonstrationSize,unfilteredOdometrieCoordinates[1]*demonstrationSize,label=\"unfiltered odometrie\")                                   \n",
    "plt.plot(afterKalmanCoordinates[0]*demonstrationSize,afterKalmanCoordinates[1]*demonstrationSize,label=\"after Filtring\")\n",
    "plt.imshow(low_res_img)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "cv2.VideoCapture(cameraIndex).release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in analyzing the image is to cluster the background. The method implemented here is an accumulation clustering, where a random initial point is picked. The eight neighbors, as well as the point itself, are compared to a lower and upper threshold. Since we are dealing with a white background, the upper threshold is set as 255. \n",
    "The advantage of this method is that it deals with bad lighting conditions and noisy webcam data. It is based on clustering 8 points at a time, without dealing with the overall luminosity, but only using the relative grey level. Only the points within the 2nd and 98th percentile are taken into account.\n",
    "Once a point is deemed to be a part of the background, its value is set at 255. The resulting map will be a “binary” image (0 and 255), with 0 being the features that the software is interested in (corners, Thymio, endpoint, obstacles).\n",
    "If the corners (next step in vision) are not detected, then the webcam gave a corrupt image, and the process is repeated (This part of the code is only implemented in the main execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bg_clustering(image, px_zero,threshold_bg):\n",
    "    list_p = [] # Place holder\n",
    "    output = np.zeros_like(image) # place holder output img\n",
    "    list_p.append((px_zero[0], px_zero[1])) # Get our initial background pixel picked\n",
    "    while len(list_p):\n",
    "        if len(list_p)<1: # sanity check to have a starting point\n",
    "            break\n",
    "        current_px = list_p[0] # Get the first pixel\n",
    "        output[current_px[0], current_px[1]] = 255 # make it 255\n",
    "        for coord in get_8_neighbors(current_px[0], current_px[1], image.shape): # Get the 8 neighbors of this pixel\n",
    "            if abs((int(image[coord[0], coord[1]])))>threshold_bg and output[coord[0], coord[1]]<255: # If each of this neighbor is above a threshold, then its a background pixel\n",
    "                output[coord[0], coord[1]] = 255 # Convert it to a 255 pixel\n",
    "                list_p.append((coord[0], coord[1])) # append it to the list of background pixels\n",
    "        list_p.pop(0) # Remove the initial pixel guess (in case we picked a wrong pixel). If its background, it will be picked later anyway\n",
    "    return output\n",
    "\n",
    "def get_8_neighbors(y, x, shape):\n",
    "    out = [] # Matrix that will have the 8 neighbors\n",
    "    # Get the 8 neighbors, unless its out of the picture borders\n",
    "    if y-1 > 0 and x-1 > 0:\n",
    "        out.append( (y-1, x-1))\n",
    "    if y-1 > 0 :\n",
    "        out.append( (y-1, x))\n",
    "    if y-1 > 0 and x+1 < shape[1]:\n",
    "        out.append( (y-1, x+1))\n",
    "    if x-1 > 0:\n",
    "        out.append( (y, x-1))\n",
    "    if x+1 < shape[1]:\n",
    "        out.append( (y, x+1))\n",
    "    if y+1 < shape[0] and x-1 > 0:\n",
    "        out.append( ( y+1, x-1))\n",
    "    if y+1 < shape[0] :\n",
    "        out.append( (y+1, x))\n",
    "    if y+1 < shape[0] and x+1 < shape[1]:\n",
    "        out.append( (y+1, x+1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner detection\n",
    "Once this image is obtained, the vision component looks for the corners. This is done using a binary mask matched to the binary image obtained when clustering the background.\n",
    "The mask - a matrix similar to the upper left corner - is matched multiple times, and the best suited result is taken as the location of the mask. Then, the template is rotated 90 degrees, to find the second corner, and then the third and the fourth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_detection(img,mask):\n",
    "    large_image = np.copy(img)\n",
    "    small_image = np.copy(mask)\n",
    "    method = cv2.TM_SQDIFF_NORMED # Method used for matching the template\n",
    "    corner_location = np.zeros((4,2)) # Place holder for the location of the corners\n",
    "    small_image = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY) # Convert to grayscale\n",
    "    _, large_image = cv2.threshold(large_image, 40, 255, cv2.THRESH_BINARY) # Get binary image\n",
    "    _, small_image = cv2.threshold(small_image, 40, 255, cv2.THRESH_BINARY) # Get binary image\n",
    "    for i in range (0,4): # Do this 4 times, one time for each corner\n",
    "        result = cv2.matchTemplate(large_image,small_image, method) # Find the corner in the image\n",
    "        mn,_,mnLoc,_ = cv2.minMaxLoc(result) # Get the best match out of the results\n",
    "        MPx,MPy = mnLoc # Extract the coordinates of the best match\n",
    "        trows,tcols = small_image.shape[:2] # Get the size of the mask\n",
    "        cv2.rectangle(large_image, (MPx,MPy),(MPx+tcols,MPy+trows),(0,0,255),2) # Draw the rectangle on large_image\n",
    "        new_img = 255*np.ones(large_image.shape,np.uint8) # Place holder for the image this is only used to draw the mask on the image for debugging\n",
    "        large_image[MPy:MPy+trows,MPx:MPx+trows] = new_img[MPy:MPy+trows,MPx:MPx+trows] # Draw the mask on the image\n",
    "        small_image = cv2.rotate(small_image, cv2.ROTATE_90_CLOCKWISE) # Ritate the mask 90 degrees to match the next corner\n",
    "        if i == 0: # These if conditions are to account for the rotation of the rectangle (not square) mask and get accurate coordinates\n",
    "            corner_location[i,:] = [MPy,MPx]\n",
    "        elif i == 1:\n",
    "            corner_location[i,:] = [MPy,MPx+tcols]\n",
    "        elif i == 2:\n",
    "            corner_location[i,:] = [MPy+trows,MPx+tcols]\n",
    "        else:\n",
    "            corner_location[i,:] = [MPy+trows,MPx]\n",
    "    corner_location = np.fliplr(corner_location) # Flip the array to get the requested shape\n",
    "    return corner_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case, for a reason or another, the corners are not ordered properly, the function makes sure these 4 corners are ordered as such: Top Left, Top Right, Bottom Right, Bottom Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    four_points = np.zeros((4, 2), dtype = \"float32\")\n",
    "    s = pts.sum(axis = 1)\n",
    "    four_points[0] = pts[np.argmin(s)]\n",
    "    four_points[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    four_points[1] = pts[np.argmin(diff)]\n",
    "    four_points[3] = pts[np.argmax(diff)]\n",
    "    return four_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Straightening \n",
    "Once these four corners are obtained, the transformation matrix is calculated.\n",
    "To do that, the 4 points are used to obtain the distances from these corners to the adjacent ones. This step “crops” the image, keeping only what is delimited by the map.\n",
    "Finally, the cv2 function cv2.getPerspectiveTransform gives us the transformation matrix M.\n",
    "M is then applied on the entire image to extract the obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_point_transform(img, pts):\n",
    "    four_points = order_points(pts) # Just in case the corners are not in the correct order\n",
    "    (top_left, top_right, bottom_right, bottom_left) = four_points # Get each corner\n",
    "\n",
    "    width_low = np.sqrt(((bottom_right[0] - bottom_left[0]) ** 2) + ((bottom_right[1] - bottom_left[1]) ** 2)) # Get the width of the lower part of the paper\n",
    "    width_high = np.sqrt(((top_right[0] - top_left[0]) ** 2) + ((top_right[1] - top_left[1]) ** 2)) # Get the width of the upper part of the paper\n",
    "    height_right = np.sqrt(((top_right[0] - bottom_right[0]) ** 2) + ((top_right[1] - bottom_right[1]) ** 2)) # Get the height of the left part of the paper\n",
    "    height_left = np.sqrt(((top_left[0] - bottom_left[0]) ** 2) + ((top_left[1] - bottom_left[1]) ** 2)) # Get the height of the right part of the paper\n",
    "\n",
    "    Width = max(int(width_low), int(width_high))\n",
    "    Height = max(int(height_right), int(height_left))\n",
    "\n",
    "    dimention_p = np.array([[0, 0],[Width - 1, 0],[Width - 1, Height - 1],[0, Height - 1]], dtype = \"float32\") # Get the location/dimension of the projection\n",
    "    M = cv2.getPerspectiveTransform(four_points, dimention_p) # Get the transformation matrix\n",
    "    img_straighten = cv2.warpPerspective(img, M, (Width, Height)) # Get the straighten image\n",
    "    plt.imshow(img_straighten)\n",
    "    plt.show()\n",
    "    return img_straighten, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function for the corner detection code and the image straightening code:\n",
    "\n",
    "<img src=\"documentation/ImagesForDocumentation/DocuMap.jpg\"\n",
    "     alt=\"Thymio Preparation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to test the function\n",
    "threshold_bg = 130\n",
    "img = cv2.imread('documentation/ImagesForDocumentation/DocuMap.jpg')\n",
    "mask = cv2.imread('documentation/ImagesForDocumentation/CornerMask.JPG')\n",
    "img[:, :, [0, 2]] = img[:, :, [2, 0]]\n",
    "p2_1, p98_1 = np.percentile(img, (2, 98))\n",
    "img_res1 = exposure.rescale_intensity(img, in_range=(p2_1,p98_1))\n",
    "img1_gray = cv2.cvtColor(img_res1, cv2.COLOR_BGR2GRAY)\n",
    "output = bg_clustering(img1_gray, (50,50),threshold_bg)\n",
    "plt.imshow(output)\n",
    "plt.show()\n",
    "corner_location = corner_detection(output,mask)\n",
    "img_straighten, M = four_point_transform(img, corner_location) # Get the transformation matrix and the straightened img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obstacles Processing\n",
    "The straightened map obtained in the “Image Straightening” is fed into the function that gets the map of the black obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obstacles(img):\n",
    "    obstacles = black_contours(img,0.001) # Get the obstacles map\n",
    "    fat_obstacles = process_obstacles(obstacles) # Clean the image and increase the size of the obstacles\n",
    "    return fat_obstacles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function black_contours get the black contours delimiting the obstacles on the map.\n",
    "The method used is a contour finding method, where only the biggest contours are taken. This removes the noise and other unwanted information from the image.\n",
    "Note that the biggest contour is removed, which is the outer borders of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_contours(img,constant):\n",
    "    # obstacles = find_contours(img_straighten_grey, 0.001) #OR\n",
    "    output = np.copy(img) #copy the image\n",
    "    output_grey = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY) # convert to gray\n",
    "    output_grey = output_grey.astype(np.uint8) #uint8 type to use as binary image\n",
    "    _, threshold = cv2.threshold(output_grey, 100, 255, cv2.THRESH_BINARY) # threshold and obtain 0 and 255 values only\n",
    "    contours,_ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # find contours in the image\n",
    "    output_grey_contours = np.zeros([output_grey.shape[0],output_grey.shape[1]]) # get a placeholder image for the contours\n",
    "\n",
    "    largest_areas = sorted(contours, key=cv2.contourArea) # sort the contours from smallest to largest\n",
    "    largest_areas = largest_areas[:-1] # remove the border of the paper (biggest area)\n",
    "    largest_areas = largest_areas[::-1] # flip the array and make it largest to smallest\n",
    "    for cnt in largest_areas[:3]:\n",
    "        approx = cv2.approxPolyDP(cnt, constant*cv2.arcLength(cnt, True), True) # Approximate the contour(s)\n",
    "        cv2.drawContours(output_grey_contours, [approx], 0, (255), thickness=cv2.FILLED) #draw the contour(s) on the place holder  # replace thickness=cv2.FILLED with thickness=5 for edges only\n",
    "        x = approx.ravel()[0] # get x coordinate of contour point\n",
    "        y = approx.ravel()[1] # get y coordinate of contour point\n",
    "    return output_grey_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the black obstacles are processed by eroding then dilating to remove unwanted noise that passed through all the previous noise filtering methods. This also closes the open areas and removes the isolated pixels.\n",
    "Secondly, the contours are increased in size to account for the size of the Thymio: i.e. to make sure the center of the robot is farther from the black contours by its width/2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obstacles(img):\n",
    "    output = np.copy(img)\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    output = cv2.erode(output, kernel, iterations=1) # Erode and delate to remove isolated pixels and close the shapes\n",
    "    output = cv2.dilate(output, kernel, iterations=2)\n",
    "    kernel2 = np.ones((9,9), np.uint8)\n",
    "    output = cv2.dilate(output, kernel2, iterations=2) # Increase to size of the obstacles to account for the size of the thymio in the path planning\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to test the function\n",
    "obstacles = get_obstacles(img_straighten)\n",
    "plt.imshow(obstacles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of endpoint and of Thymio\n",
    "Once the map is defined and the obstacles are detected, the program moves to online/live computations.\n",
    "To obtain the Thymio and the endpoint locations, similar techniques are developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thymio_info(img,M,im_dim,dim):\n",
    "    kernel = np.ones((5,5),np.float32)/25 # Get a kernel to filter\n",
    "    img = cv2.filter2D(img,-1,kernel) # Smooth (blur) the image to reduce noise\n",
    "    obstacles = get_obstacles(img) # Get the obstacles on the tilted image\n",
    "    img[obstacles == [255]] = [0,0,0] # Remove the obstacles from our current image\n",
    "    thymio_map = color_filtering(img,\"blue\") # Apply a blue filter to keep the dots on the thymio\n",
    "    thymio_coords = end_point_start_point(thymio_map, 0.001, \"thymio\") # Get the thymio position (x,y of the centers of the two blue circles on the thymio)\n",
    "    if (thymio_coords[0][0] > 0): # if the thymio is not properly detected\n",
    "        bigpt = thymio_coords[0] # Coords of the bigger circle on the thymio\n",
    "        smallpt = thymio_coords[1] # Coords of the smaller circle on the thymio\n",
    "        thymio_coord_big = tranformation_matrix(bigpt,M) # Get the location of the big circle in the straight image\n",
    "        thymio_coord_small = tranformation_matrix(smallpt,M) # Get the location of the small circle in the straight image\n",
    "        thymio_center_coord, thymio_orientation = orientation_location_thymio(thymio_coord_big, thymio_coord_small) # Get the orientation of the thymio (angle)\n",
    "        thymio_center_coord = transformation_downgrade_coords(thymio_center_coord,im_dim,dim) # Get the coordinates in the small resolution image\n",
    "        thymio_coord = [thymio_center_coord, thymio_orientation] # Concatinate the data\n",
    "        print(\"Thymio Coordinates + Orientation: \", thymio_coord)\n",
    "        return thymio_coord\n",
    "    else:\n",
    "        thymio_coords = [(-1,-1), float(\"nan\")]\n",
    "        print(\"Thymio not detected by Vision\")\n",
    "        return thymio_coords\n",
    "\n",
    "def get_endpoint_info(img,M,im_dim,dim):\n",
    "    endpoint_map = color_filtering(img,\"green\") # Apply a blue filter to keep the dots on the thymio\n",
    "    endpoint_coord = end_point_start_point(endpoint_map, 0.001, \"endpoint\") # Get the endpoint position (x,y of the center of the star)\n",
    "    if (endpoint_coord[0] > 0):\n",
    "        endpoint_coord = tranformation_matrix(endpoint_coord,M) # Get the endpoint in the straight image\n",
    "        endpoint_coord = transformation_downgrade_coords(endpoint_coord,im_dim,dim) # Get the coordinates in the small resolution image\n",
    "        print(\"Endpoint Coordinates: \", endpoint_coord)\n",
    "        return endpoint_coord\n",
    "    else:\n",
    "        endpoint_coord = [(-1,-1)]\n",
    "        print(\"Endpoint not detected by Vision\")\n",
    "        return thymio_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a new image, and do not straighten. This method is used to minimize computations as much as possible.\n",
    "\n",
    "Second, after smoothing the images, the function “get_obstacles” is used to detect the obstacles from this new warped image and removed from it (This is done to reject reflections of the light on the obstacles).\n",
    "\n",
    "Third, a colored filter is applied to a new image (Blue for Thymio, Green for Endpoint).To deal with variations of the lighting conditions, a range of values for blue or green was given to create a mask that filters the image, removing all the pixels that do not have the respective color the code is looking for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_filtering(img,color):\n",
    "    large_image = np.copy(img)\n",
    "    if color == \"blue\":\n",
    "        lower = np.array([0,0,80]) # lower color threhsold\n",
    "        upper = np.array([100,100,255]) # upper color threshold\n",
    "    elif color == \"green\":\n",
    "        lower = np.array([0,90,0]) # lower color threhsold\n",
    "        upper = np.array([90,255,130]) # upper color threshold\n",
    "    color_mask = cv2.inRange(large_image, lower, upper) # Create the mask with lower and upper threshold of RGB values\n",
    "    large_image = cv2.bitwise_and(large_image, large_image, mask=color_mask) # Bitwise and to filter the pixels that are not of the desired color\n",
    "    return large_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to test the function\n",
    "outcolfilt = color_filtering(img,\"green\")\n",
    "plt.imshow(outcolfilt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, to the color filtered image, the contours are found. Since the image was denoised multiple times, and the only remaining features are the Thymio/Endpoint, finding the contours in the image will give the contours of the feature we are looking for. However, just in case noise manages to get through, the code only takes the biggest contours as the interesting portions of the image. Since noise has a high frequency, it will never be taken as the contour.\n",
    "If the Thymio is not detected, the function returns -1 so that the state machine knows that the function failed\n",
    "Then, the center of the contours obtained is calculated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_point_start_point(img,constant,point,clust=1):\n",
    "    # obstacles = find_contours(img_straighten_grey, 0.001) #OR\n",
    "    output = np.copy(img) #copy the image\n",
    "    output_grey = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY) # convert to gray\n",
    "    output_grey = output_grey.astype(np.uint8) #uint8 type to use as binary image\n",
    "    _, threshold = cv2.threshold(output_grey, 30, 255, cv2.THRESH_BINARY) # threshold and obtain 0 and 255 values only\n",
    "    contours,_ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # find contours in the image\n",
    "    output_grey_contours = np.zeros([output_grey.shape[0],output_grey.shape[1]]) # get a placeholder image for the contours\n",
    "    largest_areas = sorted(contours, key=cv2.contourArea) # sort the contours from smallest to largest\n",
    "    if not clust: # if we are clustering, the outer boarder is removed automatically\n",
    "        largest_areas = largest_areas[:-1] # remove the outer border of the picture\n",
    "    largest_areas = largest_areas[::-1] # flip the array and make it largest to smallest\n",
    "    if (point == \"thymio\"):\n",
    "        largest_areas = largest_areas[:2] # keep only the two largest contours corresponding to the interesting parts (and remove the noisy outputs)\n",
    "        coordinates = np.zeros((2,2)) # place holder for the coordinates\n",
    "    elif(point == \"endpoint\"):\n",
    "        largest_areas = largest_areas[:1] # keep only the largest contour (endpoint)\n",
    "        coordinates = np.zeros((1,2)) # place holder for the coordinates\n",
    "    for cnt in largest_areas:\n",
    "        approx = cv2.approxPolyDP(cnt, constant*cv2.arcLength(cnt, True), True) # Approximate the contour(s)\n",
    "        cv2.drawContours(output_grey_contours, [approx], 0, (255), thickness=cv2.FILLED) #draw the contour(s) on the place holder  # replace thickness=cv2.FILLED with thickness=5 for edges only\n",
    "        x = approx.ravel()[0] # get x coordinate of contour point\n",
    "        y = approx.ravel()[1] # get y coordinate of contour point\n",
    "    location_image = np.zeros([output_grey.shape[0],output_grey.shape[1]]) # Place holder for the\n",
    "    i = 0 # index\n",
    "    for c in largest_areas:\n",
    "        try:\n",
    "            M = cv2.moments(c) # calculating moments for each contour, i.e center of the circle englobing the contours\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"]) # calculate x coordinate of center\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"]) # calculate y coordinate of center\n",
    "            cv2.circle(location_image, (cX, cY), 5, (255, 255, 255), -1) # Draw the circle englobing the contours\n",
    "        except ZeroDivisionError as err: # If the thymio is not detcted\n",
    "            coordinates = [[-1,-1],[-1,-1]]\n",
    "            break\n",
    "        if (point == \"thymio\"):\n",
    "            coordinates[i] = [cX, cY] # Assign coordinates\n",
    "        else:\n",
    "            coordinates = [cX, cY] # Assign coordinates\n",
    "        i = i + 1\n",
    "    if(point == \"thymio\"): # if we are getting garbage as location of the thymio\n",
    "        if (abs(coordinates[0][0]-coordinates[1][0]) > 40 or abs(coordinates[0][0]-coordinates[1][0]) > 40):\n",
    "            coordinates = [[-1,-1],[-1,-1]]\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we initially took an image that is not cropped nor straighten, the transformation matrix obtained when reprojecting the initial image used to get the contours is used as an affine transformation (Rotation + Translation) for the endpoint and the two centers of the circles on the Thymio.\n",
    "This simple 2x2 matrix multiplication makes the process of calculating the exact location of the robot much less computationally expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranformation_matrix(pt,M):\n",
    "    A = M[0:2,0:2]; # Rotation Matrix\n",
    "    b = M[0:2,2]; # Translation Matrix\n",
    "    tranformed_pt = np.matmul(A,pt) + b # Affine Tranformation from tilted image to straight img\n",
    "    return tranformed_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the location of the endpoint is obtained. For the Thymio, additional computations are required.\n",
    "Since the centers of the two circles on the Thymio are obtained, the next step is to get the orientation of the robot.\n",
    "To do that, the already sorted center of the circles on the Thymio (the contours found are sorted from largest to smallest) are used to calculate the slope of the line that joins them, which is then used to find the angle the robot has. The center of the thymio is also found in the same function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_location_thymio(bigpt, smallpt):\n",
    "\n",
    "    center_thymio = [(bigpt[0]+smallpt[0])/2, (bigpt[1] + smallpt[1])/2] # Getting x and y of image (points are already sorted)\n",
    "    slope = (bigpt[1]-smallpt[1])/(-bigpt[0]+smallpt[0]) # Obtain the slope of the thymio\n",
    "    angle = math.degrees(math.atan(slope)) # Get the angle in degrees of the thymio\n",
    "    if bigpt[0]<smallpt[0]:\n",
    "        if bigpt[1]<smallpt[1]:\n",
    "            print(1)\n",
    "\n",
    "    if bigpt[0]>smallpt[0]: # Convert to the appropriate quadrant\n",
    "        if bigpt[1]<smallpt[1]:\n",
    "            angle = angle - 180\n",
    "        else:\n",
    "            angle = angle + 180\n",
    "    return center_thymio, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both the coordinates of the endpoint and then location of the Thymio are converted into the scale used later in the path planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_downgrade_coords(pt,im_dim,dim):\n",
    "    x_new = pt[0]*im_dim[0]/dim[0] # convert x from the higher resolution to the lower resolution images\n",
    "    y_new = pt[1]*im_dim[1]/dim[1] # convert y from the higher resolution to the lower resolution images\n",
    "    return x_new,y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to test the function\n",
    "im_dim = img_straighten.shape\n",
    "dimension_paper = [118.9,84.1] #cm A0\n",
    "dim = (int(dimension_paper[1]),int(dimension_paper[0]))\n",
    "\n",
    "thymio_coord = get_thymio_info(img,M,dim,im_dim)\n",
    "endpoint_coord = get_endpoint_info(img,M,dim,im_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having generated the map inside the computer with vision pathplanning is needed to create a path from the starting point to the goal. This is made possible by the pathplanning class.\n",
    "This class is used to create a pathplanning object, which is linked to a specific map, on which then all pathplanning is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise path planner\n",
    "To create the object the constructor of the class needs 3 things, The occupancy grid in form of a numpy array, with which number the occupied cells are marked and many CM represents one pixel(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:52:51.473548Z",
     "start_time": "2020-08-29T12:52:51.462661Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imports needed to run pathplanning\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#import pathplanning class\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from pathPlanning import pathPlaning\n",
    "\n",
    "#load a testing map from image and convert it into a numpy array \n",
    "#(based on: https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays)\n",
    "pil_imgray = Image.open('Images/obstaclesTestMap.jpg').convert('LA')\n",
    "img = np.array(list(pil_imgray.getdata(band=0)), float)\n",
    "img.shape = (pil_imgray.size[1], pil_imgray.size[0])\n",
    "img=img<200\n",
    "occupancyGrid=img.astype(int)\n",
    "plt.imshow(occupancyGrid)\n",
    "##generate pathplanning object for the occupancy grid generated by the testmap\n",
    "pathPlanner=pathPlaning(occupancyGrid.copy(),1,1)#occipied cells are marked with a 1 and 1 cm per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning phase\n",
    "Now that the pathplanning object has a map on which it should plan paths, the goal needs to be set by calling the \"setGoal\" method of the object and hand over a numpy array with the x,y coordinates of the goal.\\[x,y\\]. These coordinates are in cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimer=timer()    #execution time when goal is set once\n",
    "#set goal\n",
    "goal=np.array([90,15])\n",
    "pathPlanner.setGoal(goal)\n",
    "\n",
    "endTimer=timer()\n",
    "print(\"Time needed for planning goal:\",endTimer-startTimer)\n",
    "\n",
    "startTimer=timer()    #execution time when goal is set again\n",
    "#set goal\n",
    "goal=np.array([90,15])\n",
    "pathPlanner.setGoal(goal)\n",
    "\n",
    "endTimer=timer()\n",
    "print(\"Time needed for planning for the same goal again:\",endTimer-startTimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the \"setGoal\" method is called it simultaneously starts the planning phase. This does not mean that a path is planned but that the map is prepared to do fast pathplanning for each starting point one can come up with. This means that if the goal is often changed it will be a lot slower than a A*-pathplanner. If the goal stays the same and just the starting point changes often, it will be quite a bit faster. This is because in this planning phase, which is encapsulated in the private method \\_\\_generateGradient. In it a map is created, where for each grid cell its distance to the goal is calculated. This distance is a path distance, so the value saved means from the current cell you the robot has to move so many cells to reach the goal. This calculation is started from the goal and is made for every cell that is not marked occupied. The result is saved in the private distanceMap attribute, which can be accessed by a getter method \"getDistanceMap\". The occupied cells are left at the starting value of 0. This means during path generation the map needs to be checked whether the cell is occupied or not. But it also means that to make a path the path generator just has to \"roll\" down hill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the distance map visible\n",
    "distanceGrid=pathPlanner.getDistanceMap()\n",
    "fillUp=np.zeros_like(occupancyGrid)#helper to make picture 3 colors\n",
    "maxValue=np.amax(distanceGrid.transpose())   # normalize values to fit into picture\n",
    "#create picture to show  map and distance map\n",
    "pic=np.dstack((occupancyGrid,np.divide(distanceGrid,maxValue),fillUp))\n",
    "plt.imshow(pic)\n",
    "plt.scatter(goal[0],goal[1],marker=\"o\", color = 'yellow',s=200)\n",
    "plt.xlabel('X-direction')\n",
    "plt.ylabel('Y-direction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this picture,in red are the obstacles, in yellow the goal and in green the distance of the pixel to the goal in terms of how much movement would be needed to get to the goal.\n",
    "As can be seen surrounding the goal(the yellow point) are dark areas. This means those cells are very close to the goal. The greener the pixel is the longer the movement required to get to the goal from this pixel(cell). As can be seen if the goal is (90,15)(x,y) in the corner bottom left. There you can see a darker diagonal. This means if the robot is on those cells he needs to move less than if he is positioned at (80,20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Phase\n",
    "Now that the planning phase is over, the pathplanner is ready to provide paths to the goal from every free cell. To start the query phase and actually plan a path the start point needs to be provided. When the start point is provided, the class calls the private method \"\\_\\_generatePath\" to generate the path from the defined start point to the defined goal. For this the method begins at the starting point and looks for the cell, which has the smallest distance to goal of all its neighbors and puts the coordinates of that cell into the path. Of course only cells which are marked as free in the occupancy grid are checked. This goes on until th goal is reached. It is a bi comparable to rolling down a hill, since the cell from where the longest path to the goal exists is at the top of the hill.\n",
    "But this is only possible if \"setGoal\" was called atleast once before! Otherwise it can not find a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:36:31.127153Z",
     "start_time": "2020-08-29T12:36:25.042891Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "startTimer=timer()\n",
    "start=np.array([10,75])\n",
    "pathPlanner.setStart(start)\n",
    "endTimer=timer()\n",
    "print(\"Time needed for the query for a new path:\",endTimer-startTimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As may have been noticeable the query phase of the pathplanning is very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the path\n",
    "Now that both planning and query phases are complete, the path can be extracted by calling the method \"getPath\" or \"getOptimizedPath\" The \"getPath\" method returns the path as a numpy array with the first line representing the x-coordinates and the second line representing the y coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unoptimizedPath=pathPlanner.getPath()\n",
    "print(unoptimizedPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"getoptimizedPath\" method returns a path where only the points, where the robot needs to turn, are retained. The path is output in the same format as the path from \"getPath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizedPath=pathPlanner.getOptimizedPath()\n",
    "print(optimizedPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the 2 paths drawn onto the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(occupancyGrid)\n",
    "plt.plot(unoptimizedPath[0], unoptimizedPath[1], marker=\"o\", color = 'blue');\n",
    "plt.scatter(optimizedPath[0],optimizedPath[1], marker=\"o\", color = 'cyan',s=150)\n",
    "plt.xlabel('X-direction')\n",
    "plt.ylabel('Y-direction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cyan dots represent the optimized path and the blue dots represent the unoptimized path. As can be seen for the optimized path only the waypoints where the robot needs to change its orientation, are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complete pathplanning demonstration\n",
    "To see the complete Pathplanning on the testing map inaction this section can be run. To get a explanation of each part, see in the previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports needed to run pathplanning\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#import pathplanning class\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from pathPlanning import pathPlaning\n",
    "\n",
    "#load a testing map from image and convert it into a numpy array \n",
    "#(based on: https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays)\n",
    "pil_imgray = Image.open('Images/obstaclesTestMap.jpg').convert('LA')\n",
    "img = np.array(list(pil_imgray.getdata(band=0)), float)\n",
    "img.shape = (pil_imgray.size[1], pil_imgray.size[0])\n",
    "img=img<200\n",
    "occupancyGrid=img.astype(int)\n",
    "plt.imshow(occupancyGrid)\n",
    "##generate pathplanning object for the occupancy grid generated by the testmap\n",
    "pathPlanner=pathPlaning(occupancyGrid.copy(),1,1)#occipied cells are marked with a 1 and 1 cm per pixel\n",
    "\n",
    "#--set goal\n",
    "startTimer=timer()    #execution time when goal is set once\n",
    "#set goal\n",
    "goal=np.array([90,15])\n",
    "pathPlanner.setGoal(goal)\n",
    "\n",
    "endTimer=timer()\n",
    "print(\"Time needed for planning goal:\",endTimer-startTimer)\n",
    "#---set path\n",
    "startTimer=timer()\n",
    "start=np.array([10,75])\n",
    "pathPlanner.setStart(start)\n",
    "endTimer=timer()\n",
    "print(\"Time needed for the query for a new path:\",endTimer-startTimer)\n",
    "\n",
    "#--- get unoptimized path\n",
    "unoptimizedPath=pathPlanner.getPath()\n",
    "#-- get optimized Path\n",
    "optimizedPath=pathPlanner.getOptimizedPath()\n",
    "#--- draw all the components onto the same picture\n",
    "\n",
    "#make the distance map visible\n",
    "distanceGrid=pathPlanner.getDistanceMap()\n",
    "fillUp=np.zeros_like(occupancyGrid)#helper to make picture 3 colors\n",
    "maxValue=np.amax(distanceGrid.transpose())   # normalize values to fit into picture\n",
    "#create picture to show  map and distance map\n",
    "pic=np.dstack((occupancyGrid,np.divide(distanceGrid,maxValue),fillUp))\n",
    "plt.imshow(pic)\n",
    "plt.scatter(goal[0],goal[1],marker=\"o\", color = 'yellow',s=200)\n",
    "plt.xlabel('X-direction')\n",
    "plt.ylabel('Y-direction')\n",
    "plt.plot(unoptimizedPath[0], unoptimizedPath[1], marker=\"o\", color = 'blue');\n",
    "plt.scatter(optimizedPath[0],optimizedPath[1], marker=\"o\", color = 'cyan',s=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue points represent the unoptimized path, the cyan points are the optimized paths waypoint and red are the occupied cells. The green shade represents the distance of that pixel(cell) to the goal. The greener the longer is the path to the goal from that pixel(cell)\n",
    "As can be seen the path follows closely the obstacle. To prevent the robot from colliding with the obstacles, they need to be enlarged in the occupancy grid before the occupancy grid is given to the constructor. To see different behaviors the goal and start can be changed freely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local navigation, avoidance of unexpected obstacles\n",
    "<img src=\"documentation/ImagesForDocumentation/ANN.jpg\"\n",
    "     alt=\"Robotics statemachine\"\n",
    "     style=\"float: left; margin-right: 5px;width: 500px;\" />\n",
    "An Artificial Neural Network is used to operate the obstacle avoidance as it is a simple and effective way to define the Thymio’s behaviour when confronted to an obstacle. The used ANN connects the sensors to the motors with a single layer of two neurons. There are 7 available proximity sensors, so we define 7 weights wil and wir respectively for the left and the right motor. Those weights values are defined according to their respective sensor position on the robot and the neurons are sums.\n",
    "The image to the left illustrates the neurons, inputs (x), weights (w) and outputs (y) of the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of unstable camera data\n",
    "<img src=\"documentation\\ImagesForDocumentation/exampleDrawingOutlier.png\"\n",
    "     alt=\"Robotics statemachine\"\n",
    "     style=\"float: left; margin-right: 5px;width: 500px;\" />\n",
    "It was noticed, that the position of the thymio given by the camera, is sometimes unstable. That means, that the position given by the camera is not always the position of the thymio in the picture. An example can be seen in the image to the left.\n",
    "So to prevent these from propagating to the filters that follow, they are removed and replaced by odometry data instead.\n",
    "This is done by comparing every new position given by the camera to the position given one iteration before. Since the thymio has a maximum speed, if the difference between the 2 measurements is too great, the measurement is considered unstable. If the data point is considered unstable, then it is discarded, and the odometry estimate is used instead. The odometry estimate is always also calculated from the last stable position given by the camera. If the camera is signaling that it does not see the thymio, the thymio relies on the odometry as well and if the camera estimate is not stable and too far from the odometry estimate, the odometry estimate is used, but if the measurement is stable and far from the odometry estimate the camera estimate is used. \n",
    "This functionality is directly integrated into the main program.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the drawing an example of this is shown. The green dots represent the camera position estimate, the red ones the odometry position estimate. The green ones with a blue circle are the ones, which are detected as outliers and the odometry used instead. During the yellow zone the camera reported that it cant see the thymio and then the odometry is used again.\n",
    "After it is decided which source to trust, the position estimate is given to the Kalman filter estimate is obtained, from the senors. This position estimate is then given to the Kalman filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odometry\n",
    "\n",
    "This function takes as input:\n",
    " - p [1x3], the robot position and orientation X, Y and theta\n",
    " - sigma_p [3x3], the covariance matrix for all three dimensions\n",
    " - t [1x2], used to aquire the elapsed time to compute speed integration\n",
    " - MAX_SPEED = 500, maximum wheel speed\n",
    " - B, the distance between the two wheels\n",
    " - CALIB, the calibration value to convert the unitless wheel speed into [cm/s]. That value has been found by moving forward the robot for 60 cm using the odometry function, mesuring the real distance travelled and adjusting the CALIB value until it fits\n",
    " - Z [3x2] is a zeros matrix used to construct the Sigma [5x5] covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"T\" is the elapsed time between t[0] and t[1]. As t[0] has the time.time() assigned after the calculation of \"T\", \"T\" represents the period (1/frequency) at which the \"odometry\" function is called. \n",
    "The error compared to the real duration it takes for the odometry function to be called is the time elapsed between t[1] and t[0], which is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the measured wheel speed returns a signed binary value, when negative speed occurs it will returns 2^16 - absolute wheel speed. In that case, we can find the real negative speed by substracting 2^16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wheel displacements are then computed by integrating the speed (and multiplying by a calibration value to convert the speed to [cm/s]).\n",
    "Those values are used to find the displacement in global coordinates (dx,dy and d_theta) following the standards equation for a differential wheels robot. The \"dy\" displacement value is computed using the \"-\" sign to take into account the direction of the Y axis in the global coordinates system (pointing \"down\").\n",
    "Finally, the displacements are added to the current position \"p\" of the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import serial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from Thymio import Thymio\n",
    "from tqdm import tqdm\n",
    "\n",
    "#const definition\n",
    "SPEED = 200\n",
    "MAX_SPEED = 500\n",
    "\n",
    "\n",
    "def odometry(p, sigma_p, t, MAX_SPEED, B = 9.5, CALIB = 0.0315, Z = np.zeros((3,2))):\n",
    "    #get elapsed time and wheels speed\n",
    "    t[1] = time.time()\n",
    "    if t[0] == 0:\n",
    "        T = 1e-6\n",
    "    else:\n",
    "        T = np.float32(t[1]-t[0])\n",
    "    t[0] = time.time()\n",
    "    speed_l = th[\"motor.left.speed\"]\n",
    "    speed_r = th[\"motor.right.speed\"]\n",
    "    \n",
    "    #convert for negative speed\n",
    "    if speed_l > MAX_SPEED:\n",
    "        speed_l = speed_l - 2**16\n",
    "    if speed_r > MAX_SPEED:\n",
    "        speed_r = speed_r - 2**16\n",
    "        \n",
    "    # compute wheel displacement\n",
    "    ds_l = T*speed_l*CALIB\n",
    "    ds_r = T*speed_r*CALIB\n",
    "    #compute displacement\n",
    "    ds = 0.5*(ds_r + ds_l)\n",
    "    d_theta = (ds_r - ds_l)/B\n",
    "    dx = ds*math.cos(p[2] + d_theta/2)\n",
    "    dy = - ds*math.sin(p[2] + d_theta/2)\n",
    "    dp = np.array([dx, dy, d_theta])\n",
    "    p = p + dp\n",
    "    # set robot angle to 0 deg if it has made one full turn\n",
    "    if math.fabs(p[2]) >= 2*math.pi:\n",
    "        p[2] = 0\n",
    "    \n",
    "    ### standard deviation covarance matrix, see slide 21, lesson 6 of BMR\n",
    "    k = 2e-2\n",
    "    sigma_delta = np.array([[k*math.fabs(ds_r), 0                ], \n",
    "                            [0                , k*math.fabs(ds_l)]])\n",
    "    c = math.cos(p[2] + d_theta/2)\n",
    "    s = math.sin(p[2] + d_theta/2)\n",
    "    J = np.array([[(1 + ds_l)/2*c + (ds_r + ds_l)*(1 - ds_l)/4*s, (ds_r + 1)/2*c + (ds_r + ds_l)*(ds_r - 1)/4*s, 1, 0, 0], \n",
    "                  [(1 + ds_l)/2*s - (ds_r + ds_l)*(1 - ds_l)/4*c, (ds_r + 1)/2*s - (ds_r + ds_l)*(ds_r - 1)/4*c, 0, 1, 0], \n",
    "                  [(1 - ds_l)/2                                 , (ds_r - 1)/2                             , 0, 0, 1]])\n",
    "    Sigma = np.asarray(np.bmat([[sigma_p, Z], [np.transpose(Z), sigma_delta]]))\n",
    "    D = np.matmul(J, Sigma)\n",
    "    Sigma_prim = np.matmul(D, np.transpose(J))\n",
    "\n",
    "    return p, Sigma_prim, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odometry Test:\n",
    "By executing the following code you can manually move the robot using the forward, right and left buttons.\n",
    "By pressing the center button the displacement is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables definition\n",
    "Ts = 0.1\n",
    "Tw = 0.05\n",
    "p = np.zeros(3)\n",
    "t = np.array([0,0], dtype = 'float64')\n",
    "Sigma_prim = np.zeros((3,3))\n",
    "\n",
    "trace_x = []\n",
    "trace_y = []\n",
    "\n",
    "th = Thymio.serial(port=\"COM6\", refreshing_rate=Ts)\n",
    "\n",
    "# wait until connected\n",
    "while len(th.variable_description()) == 0:\n",
    "\ttime.sleep(0.5)\n",
    "\tprint(\"wating for connection...\")\n",
    "\n",
    "print(\"connected\")\n",
    "time.sleep(1)\n",
    "\n",
    "while th[\"button.center\"] == 0:\n",
    "\n",
    "    p, Sigma_prim, t = odometry(p, Sigma_prim, t, MAX_SPEED)\n",
    "    \n",
    "    #store position\n",
    "    trace_x.append(p[0])\n",
    "    trace_y.append(-p[1])\n",
    "\n",
    "    if th[\"button.forward\"] == 1:\n",
    "        th.set_var(\"motor.left.target\", SPEED)\n",
    "        th.set_var(\"motor.right.target\", SPEED)\n",
    "    elif th[\"button.right\"] == 1:\n",
    "        th.set_var(\"motor.left.target\", SPEED)\n",
    "        th.set_var(\"motor.right.target\", 2**16-SPEED)\n",
    "    elif th[\"button.left\"] == 1:\n",
    "        th.set_var(\"motor.left.target\", 2**16-SPEED)\n",
    "        th.set_var(\"motor.right.target\", SPEED)\n",
    "    else:\n",
    "        th.set_var(\"motor.left.target\", 0)\n",
    "        th.set_var(\"motor.right.target\", 0)\n",
    "        \n",
    "plt.plot(trace_x, trace_y)\n",
    "plt.xlim(-20, 20)\n",
    "plt.ylim(-20, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path following\n",
    "Using the robot's position calculated by odometry the goal is now to make it follow a path.\n",
    "That function recieves as input the position and the path [2 x nb of waypoints].\n",
    "\n",
    "The \"waypoint\" is defined as the next X and Y coordinates stored in `path` to be reached by the robot.\n",
    "\n",
    "Then the waypoint metrics are computed. `waypoint_dir`, `waypoint_dist` and `waypoint_ang` are respectively the vector from the robot to the waypoint, the distance between the robot and the waypoint and the absolute angle of `waypoint_dir`.\n",
    "\n",
    "`err_angle` is the angle from the absolute robot orientation p[2] to the waypoint angle. This value is then bounded in [-pi , pi].\n",
    "To make the robot move and go for the waypoint the \"speed_regulation\" is called. More details below.\n",
    "\n",
    "When the waypoint is reached (the distance falls under THREASHOLD), the first coordinates of \"path\" (which are the current waypoint) are popped out using the \"popcol\" function and the new first coordinates of the `path` are taken as the new waypoint.\n",
    "\n",
    "If `path` is empty, it means that the robot has reached the goal and the program exits the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_following(p, path, THREASHOLD = 0.5):\n",
    "    \n",
    "    waypoint = path[:,0]\n",
    "\n",
    "    ############### WAYPOINT METRICS ##################\n",
    "\n",
    "    waypoint_dir = waypoint-[p[0],p[1]]\n",
    "    #robot-->waypoint distance\n",
    "    waypoint_dist = math.sqrt(sum(waypoint_dir**2))\n",
    "    #robot-->waypoint angle\n",
    "    waypoint_ang = math.atan2(- waypoint_dir[1],waypoint_dir[0])\n",
    "    #relative error with robot orientation\n",
    "    err_angle = waypoint_ang - p[2]\n",
    "    #if the error angle if above 180° turn the other way\n",
    "    if err_angle > math.pi:\n",
    "        err_angle = err_angle - 2*math.pi\n",
    "    if err_angle < - math.pi:\n",
    "        err_angle = 2*math.pi + err_angle\n",
    "\n",
    "    #print some variables\n",
    "    print('position:         ', p)\n",
    "    print('waypoint:         ', waypoint)\n",
    "    print('waypoint dir:     ', waypoint_dir)\n",
    "    print('waypoint dist:    ', waypoint_dist)\n",
    "    print('waypoint angle:   ', waypoint_ang)\n",
    "    print('error angle:      ', err_angle)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #if the waypoint is reached, returns popped path and next waypoint\n",
    "    if waypoint_dist < THREASHOLD:\n",
    "        path, waypoint = popcol(path, 0)\n",
    "        #if the robot has reached the goal, exit\n",
    "        if np.size(path) == 0:\n",
    "            print('GOAL REACHED')\n",
    "            return p, path  \n",
    "\n",
    "    speed_regulation(err_angle)\n",
    "\n",
    "    return p, path\n",
    "\n",
    "def popcol(my_array,pc):\n",
    "    \"\"\" column popping in numpy arrays\n",
    "    Input: my_array: NumPy array, pc: column index to pop out\n",
    "    Output: [new_array,popped_col] \"\"\"\n",
    "    print('---------------------------------------------------------WAYPOINT REACHED')\n",
    "    i = pc\n",
    "    pop = my_array[:,i]\n",
    "    new_array = np.hstack((my_array[:,:i],my_array[:,i+1:]))\n",
    "    return [new_array,pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `speed_regulation` function recieves as input the error angle. It computes a forward speed that is proportional to teh absolute value of `1/err_angle` and a rotation speed proportional to `err_angle`.\n",
    "\n",
    "Then, those speeds are combined to compute the left and right wheel speed, converted into int16 format and saturated at the maximum speed allowed. Finally, the negatives values are managed by adding 2^16 (signed int) and each wheel speed is set on the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_regulation(err_angle, K = MAX_SPEED):\n",
    "\n",
    "    ################### regulation #####################\n",
    "    \n",
    "    ### Proportional control\n",
    "    forward_speed = K/3/(5*math.fabs(err_angle)+1)\n",
    "    rotation_speed = err_angle*K/2\n",
    "\n",
    "    #compute wheel speed speed\n",
    "    left_wheel_speed = forward_speed - rotation_speed\n",
    "    right_wheel_speed = forward_speed + rotation_speed\n",
    "    #convert to integer\n",
    "    left_wheel_speed = np.int16(left_wheel_speed)\n",
    "    right_wheel_speed = np.int16(right_wheel_speed)\n",
    "\n",
    "    #Saturate speed\n",
    "    if left_wheel_speed > MAX_SPEED:\n",
    "        left_wheel_speed = MAX_SPEED\n",
    "    if left_wheel_speed < - MAX_SPEED:\n",
    "        left_wheel_speed = - MAX_SPEED\n",
    "    if right_wheel_speed > MAX_SPEED:\n",
    "        right_wheel_speed = MAX_SPEED\n",
    "    if right_wheel_speed < - MAX_SPEED:\n",
    "        right_wheel_speed = - MAX_SPEED\n",
    "\n",
    "    #assign speed to wheel and manage negative values\n",
    "    if left_wheel_speed < 0:\n",
    "        th.set_var(\"motor.left.target\", 2**16 + left_wheel_speed)\n",
    "    else:\n",
    "        th.set_var(\"motor.left.target\", left_wheel_speed)\n",
    "    if right_wheel_speed < 0:\n",
    "        th.set_var(\"motor.right.target\", 2**16 + right_wheel_speed)\n",
    "    else:\n",
    "        th.set_var(\"motor.right.target\", right_wheel_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = np.array([[0,10,10,0,0],[0,0,-10,-10,0]])\n",
    "p = np.zeros(3)\n",
    "Sigma_prim = np.zeros((3,3))\n",
    "\n",
    "while np.size(path):\n",
    "    time.sleep(Tw)\n",
    "    p, Sigma_prim, t = odometry(p, Sigma_prim, t, MAX_SPEED)\n",
    "    p, path = path_following(p, path)\n",
    "\n",
    "    if th[\"button.center\"] == 1:\n",
    "        break\n",
    "th.set_var(\"motor.left.target\", 0)\n",
    "th.set_var(\"motor.right.target\", 0)\n",
    "time.sleep(1)\n",
    "quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The project was successfully implemented. The Thymio is reaching any destination set from any start point and with any map. The physical obstacles are avoided with the neural network, and the robot can work with vision localization only, odometry only and the combination of both vision and odometry. Furthermore, the system adapts to different lighting conditions and different initial positions and angles of the camera, as long as the entire map is showing in the picture. Finally, any outlier in the measurements (camera or odometry) are filtered out, and the overall performance is improved by a Kalman filter-estimator.\n",
    "\n",
    "The project can be improved if an extended kalman filter is used. It will not improve the performance drasticly, but it will give a more elegant output.Furthermore, when doing the path planning, velocity profiles can be dictated, with a point to point motion. Finally, a camera with shutter speed can be installed,"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "497.208px",
    "left": "281.667px",
    "right": "20px",
    "top": "62px",
    "width": "749px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
