{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Thymio-Project\" data-toc-modified-id=\"Thymio-Project-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Thymio Project</a></span></li><li><span><a href=\"#Complete-Program\" data-toc-modified-id=\"Complete-Program-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Complete Program</a></span><ul class=\"toc-item\"><li><span><a href=\"#Thymio-preparation\" data-toc-modified-id=\"Thymio-preparation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Thymio preparation</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#define-the-state-machine\" data-toc-modified-id=\"define-the-state-machine-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>define the state machine</a></span><ul class=\"toc-item\"><li><span><a href=\"#define-the-Robot-object\" data-toc-modified-id=\"define-the-Robot-object-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>define the Robot object</a></span></li><li><span><a href=\"#Define-the-statefunctions\" data-toc-modified-id=\"Define-the-statefunctions-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Define the statefunctions</a></span></li></ul></li><li><span><a href=\"#......\" data-toc-modified-id=\"......-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>......</a></span></li><li><span><a href=\"#adsadaa\" data-toc-modified-id=\"adsadaa-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>adsadaa</a></span></li></ul></li><li><span><a href=\"#........\" data-toc-modified-id=\"........-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vision</a></span></li><li><span><a href=\"#Path-planning\" data-toc-modified-id=\"Path-planning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Path planning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialise-path-planner\" data-toc-modified-id=\"Initialise-path-planner-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initialise path planner</a></span></li><li><span><a href=\"#Planning-phase\" data-toc-modified-id=\"Planning-phase-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Planning phase</a></span></li><li><span><a href=\"#Query-Phase\" data-toc-modified-id=\"Query-Phase-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Query Phase</a></span></li><li><span><a href=\"#Getting-the-path\" data-toc-modified-id=\"Getting-the-path-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Getting the path</a></span></li><li><span><a href=\"#complete-pathplanning-demonstration\" data-toc-modified-id=\"complete-pathplanning-demonstration-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>complete pathplanning demonstration</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thymio Project\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:34:35.886396Z",
     "start_time": "2020-08-29T12:34:34.933996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyserial in c:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\carl\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pyserial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section  the main execution of the program is presented and available to be executed with a thymio that is properly prepared. \n",
    "## Thymio preparation\n",
    "The thymio should have 2 blue dots on it. 1 big, where the wheels are and 1 smaller on the front part. Those dots should be on a light blocking paper which is then mounted on the thymio. Furthermore there are some lights that can't be turned off. These should also be covered as well as possible.\n",
    "\n",
    "\n",
    "<img src=\"documentation/ImagesForDocumentation/\"\n",
    "     alt=\"Thymio Preparation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "Image showing the top and side cover points\n",
    "<img src=\"documentation/ImagesForDocumentation/thymioBottom.jpg\"\n",
    "     alt=\"Thymio Preparation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "\n",
    "\n",
    "Image showing the bottom spots to cover\n",
    "<img src=\"documentation/ImagesForDocumentation/thymioBehind.jpg\"\n",
    "     alt=\"Thymio Preparation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "\n",
    "\n",
    "Image showing the behind spots to cover\n",
    "\n",
    "If all these spots are covered it wont confuse the vision system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main program consists out of three parts. One that takes data  from the different senors present. These are the camera, the horizontal facing infrared distance sensors and the internal speed measurement. These are then processed into the position and orientation of the robot and preprocessed to be able to make desicions in the second part. The second part is the state machine that takes the preprocessed data from the sensor part and decides whether it should "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import all the libraries needed to run the main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:52:42.121860Z",
     "start_time": "2020-08-29T12:52:42.112779Z"
    }
   },
   "outputs": [],
   "source": [
    "#import standart libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import serial\n",
    "import numpy as np\n",
    "from numpy import linalg as LNG \n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the script\n",
    "# with the Thymio class and all the files with the group generated functions and classes\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from Thymio import Thymio\n",
    "#import functions made by group\n",
    "from pathPlanning import pathPlaning\n",
    "import ANN\n",
    "import robot_control\n",
    "import Vision\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "from kalman_filter import kalman_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the state machine\n",
    "In this section the different states are defined as in the state machine graph shown earlier. The functions in this sections are named like the states they represent and they wrap the functions imported from the pathPlanning.py, test_vision.py,ANN.py and robot_control.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the Robot object\n",
    "This object contains all the variables needed in the state machine to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stateNames_t:\n",
    "    def __init__(self):\n",
    "        self.planAcquired='planAcquired'\n",
    "        self.newPath='newpath'\n",
    "        self.checkingPath='checkingPath'\n",
    "        self.underWay='underWay'\n",
    "        self.obstacleAvoidance='obstalceAvoidance'\n",
    "        self.goalReached='goalReached'\n",
    "class FSMHelper:\n",
    "    def __init__(self,thymio,equalTolerance,wayPointDistance,pathplanner):\n",
    "        self.wasKidnapped=False\n",
    "        self.tolerance=equalTolerance\n",
    "        self.currentPosition=np.zeros((2,1))\n",
    "        self.pathPlanner=pathplanner\n",
    "        self.newPositionEstimate=0    \n",
    "        self.thymio=thymio      \n",
    "        self.obstacleDetected=False\n",
    "        self.doStop=False\n",
    "        self.goalReached=False\n",
    "        self.goal=np.zeros((2,1))\n",
    "        self.pathToFollow=np.array([[0],[0]])\n",
    "        self.FSMStates=stateNames_t()\n",
    "        self.straightenedImage=0\n",
    "        self.wayPointReachedDistance=wayPointDistance\n",
    "        self.followPath=False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the statefunctions\n",
    "In this section the different functions used to represent the states of the state machine are defined. Each function takes the FSMHelper object to make the decisions which state is the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planAcquired(robot):\n",
    "    if(robot.wasKidnapped==True):\n",
    "        robot.pathPlanner.setStart(robot.currentPosition[0:2])\n",
    "        return robot.FSMStates.newPath\n",
    "    else:\n",
    "        return robot.FSMStates.checkingPath\n",
    "def newPath(robot):#pathPlanner,pathToFollow\n",
    "    robot.pathToFollow=robot.pathPlanner.getOptimizedPath()\n",
    "    print(\"path after Planner\",robot.pathToFollow)\n",
    "    return robot.FSMStates.checkingPath\n",
    "    \n",
    "def checkingPath(robot):\n",
    "    #print(\"Size\",np.size(robot.pathToFollow))\n",
    "    if(np.size(robot.pathToFollow)!=0):\n",
    "        #print(\"path:\",robot.pathToFollow)\n",
    "        robot.followPath=True\n",
    "        return robot.FSMStates.underWay\n",
    "    else:\n",
    "        robot.followPath=False\n",
    "        robot.doStop=True\n",
    "        return robot.FSMStates.goalReached\n",
    "def underWay(robot):\n",
    "    if(robot.obstacleDetected==True):\n",
    "        robot.followPath=False\n",
    "        return robot.FSMStates.obstacleAvoidance\n",
    "    else:\n",
    "        return robot.FSMStates.planAcquired\n",
    "def avoidObstacle(robot):     \n",
    "    ANN.run_ann_without_memory(robot.thymio)\n",
    "    return robot.FSMStates.checkingPath \n",
    "def goalReached(robot):\n",
    "    robot.goalReached=True\n",
    "    \n",
    "    \n",
    "#define the concrete stateName object to make the dictionary for the actual state machine\n",
    "stateName=stateNames_t()\n",
    "switch = {\n",
    "    stateName.planAcquired     : planAcquired,\n",
    "    stateName.newPath          : newPath,\n",
    "    stateName.checkingPath     : checkingPath,\n",
    "    stateName.underWay         : underWay,\n",
    "    stateName.obstacleAvoidance: avoidObstacle,\n",
    "    stateName.goalReached      : goalReached,\n",
    "}\n",
    "currentState=stateName.planAcquired\n",
    "futureState=stateName.planAcquired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#possible struckture of main code chapter\n",
    "show state event diagramm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ............\n",
    "The first steps are to try to connect to the camera and the thymio and then set different decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adsadaa\n",
    "In this section the vision part is used to generate a map and then define a pathfinding object for that map. These section represents the 3 first states, since that division can be done in a Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "th=Thymio.serial(port=\"COM6\", refreshing_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "th.set_var_array(\"leds.top\", [0, 0, 0])\n",
    "th.set_var_array(\"leds.bottom.right\", [0, 0, 0])\n",
    "th.set_var_array(\"leds.bottom.left\", [0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameraIndex=1             #index 1 is the index of the specific machine used. it is highly likly that it is on other machines aswell\n",
    "cv2.namedWindow(\"preview\")\n",
    "videoCapture = cv2.VideoCapture(cameraIndex)\n",
    "if not(videoCapture.isOpened()):\n",
    "    raise Exception('could not connect to camera')\n",
    "videoCapture = cv2.VideoCapture(cameraIndex)\n",
    "exposureOfCamera=-2.1          #initial exposure\n",
    "videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera)\n",
    "videoCapture.set(cv2.CAP_PROP_SATURATION ,120)\n",
    "\n",
    "if not(videoCapture.isOpened()):\n",
    "    raise Exception('could not connect to camera')\n",
    "\n",
    "if videoCapture.isOpened(): # try to get the first frame\n",
    "    rval, frame = videoCapture.read()\n",
    "else:\n",
    "    rval = False\n",
    "    raise Exception('could not connect to camera')\n",
    "while rval:\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "    rval, frame = videoCapture.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "    elif(key== ord('w')):\n",
    "        exposureOfCamera=exposureOfCamera+0.1\n",
    "        print (videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera),exposureOfCamera)\n",
    "    elif(key== ord('s')):    \n",
    "        exposureOfCamera=exposureOfCamera-0.1\n",
    "        print (videoCapture.set(cv2.CAP_PROP_EXPOSURE,exposureOfCamera),exposureOfCamera)\n",
    "cv2.destroyWindow(\"preview\")\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'videoCapture' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d0d2ef5890df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#cv2.namedWindow(\"preview\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvideoCapture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_EXPOSURE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvideoCapture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# try to get the first frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideoCapture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'videoCapture' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#cv2.namedWindow(\"preview\")\n",
    "videoCapture.set(cv2.CAP_PROP_EXPOSURE,-2.9)\n",
    "if videoCapture.isOpened(): # try to get the first frame\n",
    "    rval, frame = videoCapture.read()\n",
    "else:\n",
    "    rval = False\n",
    "    raise Exception('could not read image!')\n",
    "#read corner mask\n",
    "mask= cv2.imread('Images/Mondamask.JPG')\n",
    "if mask.size==0:\n",
    "    raise Exception('Could not open Mask')\n",
    "dimension_paper = [118.9,84.1] #cm A0\n",
    "dim = (int(dimension_paper[1]),int(dimension_paper[0]))\n",
    "# Switching red and blue channels\n",
    "frame[:, :, [0, 2]] = frame[:, :, [2, 0]]\n",
    "mask[:, :, [0, 2]] = mask[:, :, [2, 0]]\n",
    "#plt.imshow(frame)\n",
    "\n",
    "#plt.show()\n",
    "plt.imsave('maptest.jpg',frame)\n",
    "#preprocess image data to facilitate map generation and thymio position accusition\n",
    "p2_1, p98_1 = np.percentile(frame, (2, 98))\n",
    "img_res1 = exposure.rescale_intensity(frame, in_range=(p2_1,p98_1))\n",
    "img1_gray = cv2.cvtColor(img_res1, cv2.COLOR_BGR2GRAY)\n",
    "threshold_bg=130\n",
    "\n",
    "percent = 0.9\n",
    "while True:\n",
    "        output = Vision.bg_clustering(img1_gray, (50,50),threshold_bg)\n",
    "        corner_location = Vision.corner_detection(output,mask) # Get the location of the 4 corners\n",
    "        img_straighten, M = Vision.four_point_transform(frame, corner_location) # Get the transformation matrix and the straighten img\n",
    "        if(img_straighten.shape[0] > output.shape[0]*percent and img_straighten.shape[1] > output.shape[1]*percent):\n",
    "            break\n",
    "        else:\n",
    "            rval, frame = videoCapture.read()\n",
    "            frame[:, :, [0, 2]] = frame[:, :, [2, 0]]\n",
    "            print(\"Retake Picture, Webcame iz noobs\") # Command to retake the picture from webcam # Get the transformation matrix and the straighten img\n",
    "\n",
    "im_dim = img_straighten.shape\n",
    "obstacles = Vision.get_obstacles(img_straighten) # OFFLINE\n",
    "thymio_coord = Vision.get_thymio_info(frame,M,dim,im_dim) # Do these online, and feed info to kalman filter\n",
    "endpoint_coord = Vision.get_endpoint_info(frame,M,dim,im_dim)\n",
    "low_res_img = cv2.resize(img_straighten, dsize=((dim[1], dim[0])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "occupancyGrid=cv2.resize(obstacles, dsize=(int(dim[1]), int(dim[0])))\n",
    "occupancyGrid=occupancyGrid>200\n",
    "occupancyGrid=occupancyGrid.astype(float)\n",
    "kernel2 = np.ones((7,7), np.uint8)\n",
    "occupancyGrid = cv2.dilate(occupancyGrid, kernel2, iterations=2)\n",
    "plt.imshow(occupancyGrid)\n",
    "plt.show()\n",
    "#plt.imshow(occupancyGrid)\n",
    "pathPlanner=pathPlaning(occupancyGrid.copy(),1,1)\n",
    "#give over the thymio connection,set the kidnapping distance to 10 and the tolerance for equality to 1e-6, the distance when it is considered way point reached\n",
    "robotStatus=FSMHelper(th,1e-6,0.5,pathPlanner)\n",
    "robot_control.th=th\n",
    "#print(np.asarray(endpoint_coord))\n",
    "robotStatus.pathPlanner.setGoal(np.asarray(endpoint_coord))\n",
    "#print(np.asarray(thymio_coord[0]))\n",
    "robotStatus.currentPosition=np.array([(thymio_coord[0][0]),(thymio_coord[0][1]),(thymio_coord[1])/180*np.pi])\n",
    "#print(robotStatus.currentPosition[0:2])\n",
    "robotStatus.pathPlanner.setStart(robotStatus.currentPosition[0:2])\n",
    "robotStatus.pathToFollow=robotStatus.pathPlanner.getOptimizedPath()\n",
    "estimatedRobotPose=robotStatus.currentPosition\n",
    "\n",
    "#show path on image\n",
    "originalPath=robotStatus.pathToFollow.copy() \n",
    "\n",
    "plt.imshow(low_res_img)\n",
    "plt.scatter(thymio_coord[0][0],thymio_coord[0][1])\n",
    "plt.plot(originalPath[0],originalPath[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import any qt binding",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e21a206675de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'qt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#constant kalman matrixes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2325\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2326\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2327\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-102>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3503\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_needmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mbackend_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend_module_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mclass\u001b[0m \u001b[0mbackend_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m         \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mbackend_mod\u001b[1;34m()\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mbackend_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[0mrequired_framework\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_required_interactive_framework\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_mod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_qt5agg.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend_agg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFigureCanvasAgg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m from .backend_qt5 import (\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mQtCore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQtGui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQtWidgets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_BackendQT5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFigureCanvasQT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFigureManagerQT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     NavigationToolbar2QT, backend_version)\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\backends\\backend_qt5.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0m_Backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFigureCanvasBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFigureManagerBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNavigationToolbar2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     TimerBase, cursors, ToolContainerBase, StatusbarBase, MouseButton)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqt_editor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigureoptions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfigureoptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqt_editor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_formsubplottool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUiSubplotTool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mqt_compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\backends\\qt_editor\\figureoptions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqt_compat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQtGui\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqt_editor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_formlayout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\backends\\qt_compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Failed to import any qt binding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# We should not get there.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unexpected QT_API: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQT_API\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import any qt binding"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "#constant kalman matrixes\n",
    "\"\"\"\n",
    "A = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "C = np.array([[0,0,0],[0,0,0],[0,0,0]])\n",
    "Q = np.array([[5, 0, 0],[0, 5, 0],[0,0,5]]) # Dependent on the error (can be linked to stated as velocity, but velocity is not taken as state. )\n",
    "R = np.array([[0.01,0,0],[0,0.01,0],[0,0,0.01]])\n",
    "Q_ini = Q\n",
    "X = np.array([[estimatedRobotPose[0]],[estimatedRobotPose[1]],[estimatedRobotPose[2]]], dtype = \"float32\")\n",
    "print(X.size)\n",
    "L = []\n",
    "\"\"\"\n",
    "timeElapsed=np.array([0,0], dtype = 'float64')\n",
    "#print(\"robotstartPose\",robotStatus.currentPosition)\n",
    "robotPositionUncertainty=np.zeros((3,3))\n",
    "#print(robotStatus.pathToFollow)\n",
    "currentState=stateName.planAcquired\n",
    "futureState=stateName.planAcquired\n",
    "originalPath=robotStatus.pathToFollow.copy()    #times 2 becuase the vision system operates at 0.5 cm per pixel and the pathfinding at 1 cm per pixel\n",
    "doPlotCounter=0\n",
    "deltaT=0.0\n",
    "tbefore=time.time()\n",
    "tnow=0\n",
    "unfilteredCoordinates=list()\n",
    "filteredCoordinates=list()\n",
    "cutOfDistance=5\n",
    "cameraDataStableCounter=0\n",
    "cameraEstimate=robotStatus.currentPosition\n",
    "cameraData=True\n",
    "numberOfStableMeasurments=10\n",
    "while(True):         #main execution loop\n",
    "    ##read sensors    \n",
    "    #----get robot position from camera\n",
    "    [frameCaptureSuccesfull,newPicture]=videoCapture.read()\n",
    "    if(frameCaptureSuccesfull==False):\n",
    "        raise Exception('could not read from camera')\n",
    "    newPicture[:, :, [0, 2]] = newPicture[:, :, [2, 0]]\n",
    "    thymio_coord = Vision.get_thymio_info(newPicture,M,dim,im_dim) # Do these online, and feed info to kalman filter\n",
    "    ###---------------camera data true\n",
    "    if(cameraData==True):\n",
    "        newCameraEstimate=np.array([thymio_coord[0][0],thymio_coord[0][1],thymio_coord[1]/180.0*np.pi])\n",
    "        unfilteredCoordinates.append((cameraEstimate[0],cameraEstimate[1]))\n",
    "        \n",
    "        if(LNG.norm(newCameraEstimate[0:2]-cameraEstimate[0:2])<cutOfDistance):\n",
    "            cameraDataStableCounter=cameraDataStableCounter+1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            cameraDataStableCounter=0\n",
    "        cameraEstimate=newCameraEstimate\n",
    "   \n",
    "    #print(\"cameraEstimate\",cameraEstimate,\"ThymioCoordinate\",thymio_coord)   \n",
    "    #print(\"CurrentPosition\",robotStatus.currentPosition)\n",
    "    #-----get odometrie data\n",
    "    #print(\"before odometry\",robotStatus.currentPosition)\n",
    "    estimatedRobotPose,robotPositionUncertainty,timeElapsed=robot_control.odometry(robotStatus.currentPosition,robotPositionUncertainty,timeElapsed, robot_control.MAX_SPEED)\n",
    "    \"\"\"plt.scatter(estimatedRobotPose[0],estimatedRobotPose[1])\n",
    "    plt.show()\"\"\"\n",
    "    \"\"\"\n",
    "    V_left=th[\"motor.left.speed\"]*0.0135\n",
    "    V_right=th[\"motor.right.speed\"]*0.0135\n",
    "    v_avg = (V_left + V_right)/2\n",
    "    v_delta = V_left- V_right\n",
    "    \"\"\"\n",
    "    #-----estimate current robot position\n",
    "    tnow=time.time()\n",
    "    deltaT=tnow-tbefore\n",
    "    #print(\"deltaT\",deltaT)\n",
    "    tbefore=tnow\n",
    "    \"\"\"\n",
    "    B = np.array([[(math.cos(cameraEstimate[2])),0.], [(math.sin(cameraEstimate[2])), 0.], [0.,1.]])\n",
    "    Z = np.array([[cameraEstimate[0]],[cameraEstimate[1]],[cameraEstimate[2]]], dtype = \"float32\")\n",
    "    u = np.array([[v_avg*deltaT],[v_delta*deltaT]])\n",
    "    \"\"\"\n",
    "    #print(u)\n",
    "    #thymio_position,cov = kalman_filt(X,u,Q_ini,Z,A,B,C,Q,R)\n",
    "    \n",
    "    #X = thymio_position\n",
    "    #Q_ini = cov\n",
    "    \"\"\"#data accusition\n",
    "    print(\"Camera Estimate:\",cameraEstimate)\n",
    "    print(\"leftmotor:\",th[\"motor.left.speed\"])\n",
    "    print(\"rightmotor:\",th[\"motor.right.speed\"])\n",
    "    \n",
    "    \"\"\"\n",
    "    #if(cameraDataStableCounter>numberOfStableMeasurments):\n",
    "        #print(cameraDataStableCounter)\n",
    "    if(cameraData==True and (LNG.norm(estimatedRobotPose[0:2]-cameraEstimate[0:2])<cutOfDistance or cameraDataStableCounter>numberOfStableMeasurments)):\n",
    "        robotStatus.currentPosition=cameraEstimate\n",
    "    else:\n",
    "        robotStatus.currentPosition=estimatedRobotPose #np.transpose(X).flatten()\n",
    "    filteredCoordinates.append((robotStatus.currentPosition[0],robotStatus.currentPosition[1]))\n",
    "    #print(\"stuff\",robotStatus.newPositionEstimate)\n",
    "    #------------check if unexcpected obstacle is present\n",
    "    robotStatus.obstacleDetected=not(all(sensorValues==0 for sensorValues in robotStatus.thymio[\"prox.horizontal\"]))\n",
    "    #print(\"ObstacleDetected\",robotStatus.obstacleDetected)\n",
    "    #if(cameraData==True):\n",
    "        #robotStatus.currentPosition\n",
    "   \n",
    "    \n",
    "    \n",
    "    #------------------------------make desicions and work with the collected data \n",
    "    \n",
    "    \n",
    "    \n",
    "    stateToExecute=switch.get(currentState)\n",
    "    #print(currentState)\n",
    "    futureState=stateToExecute(robotStatus)\n",
    "    \n",
    "    #doRobotControl here\n",
    "    #_,robotStatus.pathToFollow=robot_control.path_following(robotStatus.currentPosition, robotStatus.pathToFollow, THREASHOLD = 0.5)\n",
    "    \n",
    "    #stopping robot if goal reached end programm\n",
    "    if(robotStatus.followPath==True and np.size(robotStatus.pathToFollow)!=0):\n",
    "        _,robotStatus.pathToFollow=robot_control.path_following(robotStatus.currentPosition,robotStatus.pathToFollow)\n",
    "    if(robotStatus.doStop==True):\n",
    "        robotStatus.thymio.set_var(\"motor.left.target\", 0)\n",
    "        robotStatus.thymio.set_var(\"motor.right.target\", 0)\n",
    "    if(robotStatus.goalReached==True):\n",
    "        robotStatus.thymio.set_var(\"motor.left.target\", 0)\n",
    "        robotStatus.thymio.set_var(\"motor.right.target\", 0)\n",
    "        break\n",
    "    currentState=futureState\n",
    "    doPlotCounter=doPlotCounter+1\n",
    "    if(doPlotCounter>10 or robotStatus.goalReached==True):\n",
    "        doPlotCounter=0\n",
    "        low_res_img = cv2.resize(newPicture, dsize=((dim[1], dim[0])))\n",
    "        low_res_img[:, :, [0, 2]] = low_res_img[:, :, [2, 0]]\n",
    "        low_res_img = cv2.circle(low_res_img, (int(robotStatus.currentPosition[0]),int(robotStatus.currentPosition[1])), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "        cv2.circle(low_res_img, (int(cameraEstimate[0]),int(cameraEstimate[1])), radius=10, color=(0, 255, 0), thickness=2)\n",
    "        cv2.circle(low_res_img, (int(estimatedRobotPose[0]),int(estimatedRobotPose[1])), radius=8, color=(255, 0, 0), thickness=2)\n",
    "        cv2.circle(low_res_img, (int(robotStatus.currentPosition[0]),int(robotStatus.currentPosition[1])), radius=16, color=(255, 0, 0), thickness=2)\n",
    "        cv2.imshow(\"Display window\",low_res_img)\n",
    "        if(robotStatus.goalReached):\n",
    "            cv2.waitKey(0)\n",
    "        else:\n",
    "            cv2.waitKey(1)\n",
    "        #plt.scatter(thymio_coord[0][0],thymio_coord[0][1])\n",
    "        #plt.plot(originalPath[0],originalPath[1])\n",
    "        #print(\"odometrie estimate\",estimatedRobotPose)\n",
    "        #plt.scatter(estimatedRobotPose[0],estimatedRobotPose[1])\n",
    "        #plt.show()\n",
    "unfilteredCoordinates=np.array(unfilteredCoordinates).reshape(-1, 2).transpose()\n",
    "filteredCoordinates=np.array(filteredCoordinates).reshape(-1, 2).transpose()\n",
    "plt.plot(unfilteredCoordinates[0],unfilteredCoordinates[1],label=\"Measured\")\n",
    "plt.plot(filteredCoordinates[0],filteredCoordinates[1],label=\"filtered\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "cv2.destroyAllWindows() \n",
    "cv2.VideoCapture(cameraIndex).release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5c70c06b3644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcameraIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.destroyAllWindows() \n",
    "cv2.VideoCapture(cameraIndex).release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b0221d21fc0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"motor.left.target\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m00\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"motor.right.target\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m00\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#ANN.run_ann_without_memory(th)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'th' is not defined"
     ]
    }
   ],
   "source": [
    "th.set_var(\"motor.left.target\", 00)\n",
    "th.set_var(\"motor.right.target\",00)\n",
    "#ANN.run_ann_without_memory(th)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in analyzing the image is to cluster the background. The method implemented here is an accumulation clustering, where a random initial point is picked. The eight neighbors, as well as the point itself, are compared to a lower and upper threshold. Since we are dealing with a white background, the upper threshold is set as 255. \n",
    "The advantage of this method is that it deals with bad lighting conditions and noisy webcam data. It is based on clustering 8 points at a time, without dealing with the overall luminosity, but only using the relative grey level. Only the points within the 2nd and 98th percentile are taken into account.\n",
    "Once a point is deemed to be a part of the background, its value is set at 255. The resulting map will be a “binary” image (0 and 255), with 0 being the features that the software is interested in (corners, Thymio, endpoint, obstacles).\n",
    "If the corners (next step in vision) are not detected, then the webcam gave a corrupt image, and the process is repeated (This part of the code is only implemented in the main execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bg_clustering(image, px_zero,threshold_bg):\n",
    "    list_p = [] # Place holder\n",
    "    output = np.zeros_like(image) # place holder output img\n",
    "    list_p.append((px_zero[0], px_zero[1])) # Get our initial background pixel picked\n",
    "    while len(list_p):\n",
    "        if len(list_p)<1: # sanity check to have a starting point\n",
    "            break\n",
    "        current_px = list_p[0] # Get the first pixel\n",
    "        output[current_px[0], current_px[1]] = 255 # make it 255\n",
    "        for coord in get_8_neighbors(current_px[0], current_px[1], image.shape): # Get the 8 neighbors of this pixel\n",
    "            if abs((int(image[coord[0], coord[1]])))>threshold_bg and output[coord[0], coord[1]]<255: # If each of this neighbor is above a threshold, then its a background pixel\n",
    "                output[coord[0], coord[1]] = 255 # Convert it to a 255 pixel\n",
    "                list_p.append((coord[0], coord[1])) # append it to the list of background pixels\n",
    "        list_p.pop(0) # Remove the initial pixel guess (in case we picked a wrong pixel). If its background, it will be picked later anyway\n",
    "    return output\n",
    "\n",
    "def get_8_neighbors(y, x, shape):\n",
    "    out = [] # Matrix that will have the 8 neighbors\n",
    "    # Get the 8 neighbors, unless its out of the picture borders\n",
    "    if y-1 > 0 and x-1 > 0:\n",
    "        out.append( (y-1, x-1))\n",
    "    if y-1 > 0 :\n",
    "        out.append( (y-1, x))\n",
    "    if y-1 > 0 and x+1 < shape[1]:\n",
    "        out.append( (y-1, x+1))\n",
    "    if x-1 > 0:\n",
    "        out.append( (y, x-1))\n",
    "    if x+1 < shape[1]:\n",
    "        out.append( (y, x+1))\n",
    "    if y+1 < shape[0] and x-1 > 0:\n",
    "        out.append( ( y+1, x-1))\n",
    "    if y+1 < shape[0] :\n",
    "        out.append( (y+1, x))\n",
    "    if y+1 < shape[0] and x+1 < shape[1]:\n",
    "        out.append( (y+1, x+1))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner detection\n",
    "Once this image is obtained, the vision component looks for the corners. This is done using a binary mask matched to the binary image obtained when clustering the background.\n",
    "The mask - a matrix similar to the upper left corner - is matched multiple times, and the best suited result is taken as the location of the mask. Then, the template is rotated 90 degrees, to find the second corner, and then the third and the fourth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_detection(img,mask):\n",
    "    large_image = np.copy(img)\n",
    "    small_image = np.copy(mask)\n",
    "    method = cv2.TM_SQDIFF_NORMED # Method used for matching the template\n",
    "    corner_location = np.zeros((4,2)) # Place holder for the location of the corners\n",
    "    small_image = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY) # Convert to grayscale\n",
    "    _, large_image = cv2.threshold(large_image, 40, 255, cv2.THRESH_BINARY) # Get binary image\n",
    "    _, small_image = cv2.threshold(small_image, 40, 255, cv2.THRESH_BINARY) # Get binary image\n",
    "    for i in range (0,4): # Do this 4 times, one time for each corner\n",
    "        result = cv2.matchTemplate(large_image,small_image, method) # Find the corner in the image\n",
    "        mn,_,mnLoc,_ = cv2.minMaxLoc(result) # Get the best match out of the results\n",
    "        MPx,MPy = mnLoc # Extract the coordinates of the best match\n",
    "        trows,tcols = small_image.shape[:2] # Get the size of the mask\n",
    "        cv2.rectangle(large_image, (MPx,MPy),(MPx+tcols,MPy+trows),(0,0,255),2) # Draw the rectangle on large_image\n",
    "        new_img = 255*np.ones(large_image.shape,np.uint8) # Place holder for the image this is only used to draw the mask on the image for debugging\n",
    "        large_image[MPy:MPy+trows,MPx:MPx+trows] = new_img[MPy:MPy+trows,MPx:MPx+trows] # Draw the mask on the image\n",
    "        small_image = cv2.rotate(small_image, cv2.ROTATE_90_CLOCKWISE) # Ritate the mask 90 degrees to match the next corner\n",
    "        plt.imshow(large_image)\n",
    "        plt.show()\n",
    "        if i == 0: # These if conditions are to account for the rotation of the rectangle (not square) mask and get accurate coordinates\n",
    "            corner_location[i,:] = [MPy,MPx]\n",
    "        elif i == 1:\n",
    "            corner_location[i,:] = [MPy,MPx+tcols]\n",
    "        elif i == 2:\n",
    "            corner_location[i,:] = [MPy+trows,MPx+tcols]\n",
    "        else:\n",
    "            corner_location[i,:] = [MPy+trows,MPx]\n",
    "    corner_location = np.fliplr(corner_location) # Flip the array to get the requested shape\n",
    "    return corner_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case, for a reason or another, the corners are not ordered properly, the function makes sure these 4 corners are ordered as such: Top Left, Top Right, Bottom Right, Bottom Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "    four_points = np.zeros((4, 2), dtype = \"float32\")\n",
    "    s = pts.sum(axis = 1)\n",
    "    four_points[0] = pts[np.argmin(s)]\n",
    "    four_points[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    four_points[1] = pts[np.argmin(diff)]\n",
    "    four_points[3] = pts[np.argmax(diff)]\n",
    "    return four_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Straightening \n",
    "Once these four corners are obtained, the transformation matrix is calculated.\n",
    "To do that, the 4 points are used to obtain the distances from these corners to the adjacent ones. This step “crops” the image, keeping only what is delimited by the map.\n",
    "Finally, the cv2 function cv2.getPerspectiveTransform gives us the transformation matrix M.\n",
    "M is then applied on the entire image to extract the obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_point_transform(img, pts):\n",
    "    four_points = order_points(pts) # Just in case the corners are not in the correct order\n",
    "    (top_left, top_right, bottom_right, bottom_left) = four_points # Get each corner\n",
    "\n",
    "    width_low = np.sqrt(((bottom_right[0] - bottom_left[0]) ** 2) + ((bottom_right[1] - bottom_left[1]) ** 2)) # Get the width of the lower part of the paper\n",
    "    width_high = np.sqrt(((top_right[0] - top_left[0]) ** 2) + ((top_right[1] - top_left[1]) ** 2)) # Get the width of the upper part of the paper\n",
    "    height_right = np.sqrt(((top_right[0] - bottom_right[0]) ** 2) + ((top_right[1] - bottom_right[1]) ** 2)) # Get the height of the left part of the paper\n",
    "    height_left = np.sqrt(((top_left[0] - bottom_left[0]) ** 2) + ((top_left[1] - bottom_left[1]) ** 2)) # Get the height of the right part of the paper\n",
    "\n",
    "    Width = max(int(width_low), int(width_high))\n",
    "    Height = max(int(height_right), int(height_left))\n",
    "\n",
    "    dimention_p = np.array([[0, 0],[Width - 1, 0],[Width - 1, Height - 1],[0, Height - 1]], dtype = \"float32\") # Get the location/dimension of the projection\n",
    "    M = cv2.getPerspectiveTransform(four_points, dimention_p) # Get the transformation matrix\n",
    "    img_straighten = cv2.warpPerspective(img, M, (Width, Height)) # Get the straighten image\n",
    "    plt.imshow(img_straighten)\n",
    "    plt.show()\n",
    "    return img_straighten, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function for the corner detection code and the image straightening code:\n",
    "\n",
    "<img src=\"documentation/ImagesForDocumentation/DocuMap.jpg\"\n",
    "     alt=\"Thymio Preparation\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_bg = 130\n",
    "img = cv2.imread('documentation/ImagesForDocumentation/DocuMap.jpg')\n",
    "mask = cv2.imread('documentation/ImagesForDocumentation/corner_mask.png')\n",
    "p2_1, p98_1 = np.percentile(img, (2, 98))\n",
    "img_res1 = exposure.rescale_intensity(img, in_range=(p2_1,p98_1))\n",
    "img1_gray = cv2.cvtColor(img_res1, cv2.COLOR_BGR2GRAY)\n",
    "output = bg_clustering(img1_gray, (50,50),threshold_bg)\n",
    "plt.imshow(output)\n",
    "plt.show()\n",
    "corner_location = corner_detection(output,mask)\n",
    "img_straighten, M = four_point_transform(img, corner_location) # Get the transformation matrix and the straightened img\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obstacles Processing\n",
    "The straightened map obtained in the “Image Straightening” is fed into the function that gets the map of the black obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obstacles(img):\n",
    "    obstacles = black_contours(img,0.001) # Get the obstacles map\n",
    "    fat_obstacles = process_obstacles(obstacles) # Clean the image and increase the size of the obstacles\n",
    "    return fat_obstacles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function black_contours get the black contours delimiting the obstacles on the map.\n",
    "The method used is a contour finding method, where only the biggest contours are taken. This removes the noise and other unwanted information from the image.\n",
    "Note that the biggest contour is removed, which is the outer borders of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_contours(img,constant):\n",
    "    # obstacles = find_contours(img_straighten_grey, 0.001) #OR\n",
    "    output = np.copy(img) #copy the image\n",
    "    output_grey = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY) # convert to gray\n",
    "    output_grey = output_grey.astype(np.uint8) #uint8 type to use as binary image\n",
    "    _, threshold = cv2.threshold(output_grey, 100, 255, cv2.THRESH_BINARY) # threshold and obtain 0 and 255 values only\n",
    "    contours,_ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # find contours in the image\n",
    "    output_grey_contours = np.zeros([output_grey.shape[0],output_grey.shape[1]]) # get a placeholder image for the contours\n",
    "\n",
    "    largest_areas = sorted(contours, key=cv2.contourArea) # sort the contours from smallest to largest\n",
    "    largest_areas = largest_areas[:-1] # remove the border of the paper (biggest area)\n",
    "    largest_areas = largest_areas[::-1] # flip the array and make it largest to smallest\n",
    "    for cnt in largest_areas[:3]:\n",
    "        approx = cv2.approxPolyDP(cnt, constant*cv2.arcLength(cnt, True), True) # Approximate the contour(s)\n",
    "        cv2.drawContours(output_grey_contours, [approx], 0, (255), thickness=cv2.FILLED) #draw the contour(s) on the place holder  # replace thickness=cv2.FILLED with thickness=5 for edges only\n",
    "        x = approx.ravel()[0] # get x coordinate of contour point\n",
    "        y = approx.ravel()[1] # get y coordinate of contour point\n",
    "    if Plot:\n",
    "        plt.imshow(output_grey_contours)\n",
    "        plt.show()\n",
    "    return output_grey_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the black obstacles are processed by eroding then dilating to remove unwanted noise that passed through all the previous noise filtering methods. This also closes the open areas and removes the isolated pixels.\n",
    "Secondly, the contours are increased in size to account for the size of the Thymio: i.e. to make sure the center of the robot is farther from the black contours by its width/2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_obstacles(img):\n",
    "    output = np.copy(img)\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    output = cv2.erode(output, kernel, iterations=1) # Erode and delate to remove isolated pixels and close the shapes\n",
    "    output = cv2.dilate(output, kernel, iterations=2)\n",
    "    kernel2 = np.ones((9,9), np.uint8)\n",
    "    output = cv2.dilate(output, kernel2, iterations=2) # Increase to size of the obstacles to account for the size of the thymio in the path planning\n",
    "    if Plot:\n",
    "        plt.imshow(output)\n",
    "        plt.show()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of endpoint and of Thymio\n",
    "Once the map is defined and the obstacles are detected, the program moves to online/live computations.\n",
    "To obtain the Thymio and the endpoint locations, similar techniques are developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thymio_info(img,M,im_dim,dim):\n",
    "    kernel = np.ones((5,5),np.float32)/25 # Get a kernel to filter\n",
    "    img = cv2.filter2D(img,-1,kernel) # Smooth (blur) the image to reduce noise\n",
    "    obstacles = get_obstacles(img) # Get the obstacles on the tilted image\n",
    "    img[obstacles == [255]] = [0,0,0] # Remove the obstacles from our current image\n",
    "    thymio_map = color_filtering(img,\"blue\") # Apply a blue filter to keep the dots on the thymio\n",
    "    thymio_coords = end_point_start_point(thymio_map, 0.001, \"thymio\") # Get the thymio position (x,y of the centers of the two blue circles on the thymio)\n",
    "    if (thymio_coords[0][0] > 0): # if the thymio is not properly detected\n",
    "        bigpt = thymio_coords[0] # Coords of the bigger circle on the thymio\n",
    "        smallpt = thymio_coords[1] # Coords of the smaller circle on the thymio\n",
    "        thymio_coord_big = tranformation_matrix(bigpt,M) # Get the location of the big circle in the straight image\n",
    "        thymio_coord_small = tranformation_matrix(smallpt,M) # Get the location of the small circle in the straight image\n",
    "        thymio_center_coord, thymio_orientation = orientation_location_thymio(thymio_coord_big, thymio_coord_small) # Get the orientation of the thymio (angle)\n",
    "        thymio_center_coord = transformation_downgrade_coords(thymio_center_coord,im_dim,dim) # Get the coordinates in the small resolution image\n",
    "        thymio_coord = [thymio_center_coord, thymio_orientation] # Concatinate the data\n",
    "        print(\"Thymio Coordinates + Orientation: \", thymio_coord)\n",
    "        return thymio_coord\n",
    "    else:\n",
    "        thymio_coords = [(-1,-1), float(\"nan\")]\n",
    "        print(\"Thymio not detected by Vision\")\n",
    "        return thymio_coords\n",
    "\n",
    "def get_endpoint_info(img,M,im_dim,dim):\n",
    "    endpoint_map = color_filtering(img,\"green\") # Apply a blue filter to keep the dots on the thymio\n",
    "    endpoint_coord = end_point_start_point(endpoint_map, 0.001, \"endpoint\") # Get the endpoint position (x,y of the center of the star)\n",
    "    if (endpoint_coord[0] > 0):\n",
    "        endpoint_coord = tranformation_matrix(endpoint_coord,M) # Get the endpoint in the straight image\n",
    "        endpoint_coord = transformation_downgrade_coords(endpoint_coord,im_dim,dim) # Get the coordinates in the small resolution image\n",
    "        print(\"Endpoint Coordinates: \", endpoint_coord)\n",
    "        return endpoint_coord\n",
    "    else:\n",
    "        endpoint_coord = [(-1,-1)]\n",
    "        print(\"Endpoint not detected by Vision\")\n",
    "        return thymio_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a new image, and do not straighten. This method is used to minimize computations as much as possible.\n",
    "\n",
    "Second, after smoothing the images, the function “get_obstacles” is used to detect the obstacles from this new warped image and removed from it (This is done to reject reflections of the light on the obstacles).\n",
    "\n",
    "Third, a colored filter is applied to a new image (Blue for Thymio, Green for Endpoint).To deal with variations of the lighting conditions, a range of values for blue or green was given to create a mask that filters the image, removing all the pixels that do not have the respective color the code is looking for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_filtering(img,color):\n",
    "    large_image = np.copy(img)\n",
    "    if color == \"blue\":\n",
    "        lower = np.array([0,0,80]) # lower color threhsold\n",
    "        upper = np.array([100,100,255]) # upper color threshold\n",
    "    elif color == \"green\":\n",
    "        lower = np.array([0,90,0]) # lower color threhsold\n",
    "        upper = np.array([90,255,130]) # upper color threshold\n",
    "    color_mask = cv2.inRange(large_image, lower, upper) # Create the mask with lower and upper threshold of RGB values\n",
    "    large_image = cv2.bitwise_and(large_image, large_image, mask=color_mask) # Bitwise and to filter the pixels that are not of the desired color\n",
    "    if Plot:\n",
    "        plt.imshow(large_image)\n",
    "        plt.show()\n",
    "    return large_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth, to the color filtered image, the contours are found. Since the image was denoised multiple times, and the only remaining features are the Thymio/Endpoint, finding the contours in the image will give the contours of the feature we are looking for. However, just in case noise manages to get through, the code only takes the biggest contours as the interesting portions of the image. Since noise has a high frequency, it will never be taken as the contour.\n",
    "If the Thymio is not detected, the function returns -1 so that the state machine knows that the function failed\n",
    "Then, the center of the contours obtained is calculated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_point_start_point(img,constant,point,clust=1):\n",
    "    # obstacles = find_contours(img_straighten_grey, 0.001) #OR\n",
    "    output = np.copy(img) #copy the image\n",
    "    output_grey = cv2.cvtColor(output, cv2.COLOR_RGB2GRAY) # convert to gray\n",
    "    output_grey = output_grey.astype(np.uint8) #uint8 type to use as binary image\n",
    "    _, threshold = cv2.threshold(output_grey, 30, 255, cv2.THRESH_BINARY) # threshold and obtain 0 and 255 values only\n",
    "    contours,_ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # find contours in the image\n",
    "    output_grey_contours = np.zeros([output_grey.shape[0],output_grey.shape[1]]) # get a placeholder image for the contours\n",
    "    largest_areas = sorted(contours, key=cv2.contourArea) # sort the contours from smallest to largest\n",
    "    if not clust: # if we are clustering, the outer boarder is removed automatically\n",
    "        largest_areas = largest_areas[:-1] # remove the outer border of the picture\n",
    "    largest_areas = largest_areas[::-1] # flip the array and make it largest to smallest\n",
    "    if (point == \"thymio\"):\n",
    "        largest_areas = largest_areas[:2] # keep only the two largest contours corresponding to the interesting parts (and remove the noisy outputs)\n",
    "        coordinates = np.zeros((2,2)) # place holder for the coordinates\n",
    "    elif(point == \"endpoint\"):\n",
    "        largest_areas = largest_areas[:1] # keep only the largest contour (endpoint)\n",
    "        coordinates = np.zeros((1,2)) # place holder for the coordinates\n",
    "    for cnt in largest_areas:\n",
    "        approx = cv2.approxPolyDP(cnt, constant*cv2.arcLength(cnt, True), True) # Approximate the contour(s)\n",
    "        cv2.drawContours(output_grey_contours, [approx], 0, (255), thickness=cv2.FILLED) #draw the contour(s) on the place holder  # replace thickness=cv2.FILLED with thickness=5 for edges only\n",
    "        x = approx.ravel()[0] # get x coordinate of contour point\n",
    "        y = approx.ravel()[1] # get y coordinate of contour point\n",
    "    location_image = np.zeros([output_grey.shape[0],output_grey.shape[1]]) # Place holder for the\n",
    "    i = 0 # index\n",
    "    for c in largest_areas:\n",
    "        try:\n",
    "            M = cv2.moments(c) # calculating moments for each contour, i.e center of the circle englobing the contours\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"]) # calculate x coordinate of center\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"]) # calculate y coordinate of center\n",
    "            cv2.circle(location_image, (cX, cY), 5, (255, 255, 255), -1) # Draw the circle englobing the contours\n",
    "        except ZeroDivisionError as err: # If the thymio is not detcted\n",
    "            coordinates = [[-1,-1],[-1,-1]]\n",
    "            break\n",
    "        if (point == \"thymio\"):\n",
    "            coordinates[i] = [cX, cY] # Assign coordinates\n",
    "        else:\n",
    "            coordinates = [cX, cY] # Assign coordinates\n",
    "        i = i + 1\n",
    "    if Plot:\n",
    "        plt.imshow(location_image)\n",
    "        plt.show()\n",
    "    if(point == \"thymio\"): # if we are getting garbage as location of the thymio\n",
    "        if (abs(coordinates[0][0]-coordinates[1][0]) > 40 or abs(coordinates[0][0]-coordinates[1][0]) > 40):\n",
    "            coordinates = [[-1,-1],[-1,-1]]\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we initially took an image that is not cropped nor straighten, the transformation matrix obtained when reprojecting the initial image used to get the contours is used as an affine transformation (Rotation + Translation) for the endpoint and the two centers of the circles on the Thymio.\n",
    "This simple 2x2 matrix multiplication makes the process of calculating the exact location of the robot much less computationally expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranformation_matrix(pt,M):\n",
    "    A = M[0:2,0:2]; # Rotation Matrix\n",
    "    b = M[0:2,2]; # Translation Matrix\n",
    "    tranformed_pt = np.matmul(A,pt) + b # Affine Tranformation from tilted image to straight img\n",
    "    return tranformed_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the location of the endpoint is obtained. For the Thymio, additional computations are required.\n",
    "Since the centers of the two circles on the Thymio are obtained, the next step is to get the orientation of the robot.\n",
    "To do that, the already sorted center of the circles on the Thymio (the contours found are sorted from largest to smallest) are used to calculate the slope of the line that joins them, which is then used to find the angle the robot has. The center of the thymio is also found in the same function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_location_thymio(bigpt, smallpt):\n",
    "\n",
    "    center_thymio = [(bigpt[0]+smallpt[0])/2, (bigpt[1] + smallpt[1])/2] # Getting x and y of image (points are already sorted)\n",
    "    slope = (bigpt[1]-smallpt[1])/(-bigpt[0]+smallpt[0]) # Obtain the slope of the thymio\n",
    "    angle = math.degrees(math.atan(slope)) # Get the angle in degrees of the thymio\n",
    "    if bigpt[0]<smallpt[0]:\n",
    "        if bigpt[1]<smallpt[1]:\n",
    "            print(1)\n",
    "\n",
    "    if bigpt[0]>smallpt[0]: # Convert to the appropriate quadrant\n",
    "        if bigpt[1]<smallpt[1]:\n",
    "            angle = angle - 180\n",
    "        else:\n",
    "            angle = angle + 180\n",
    "    return center_thymio, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both the coordinates of the endpoint and then location of the Thymio are converted into the scale used later in the path planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_downgrade_coords(pt,im_dim,dim):\n",
    "    x_new = pt[0]*im_dim[0]/dim[0] # convert x from the higher resolution to the lower resolution images\n",
    "    y_new = pt[1]*im_dim[1]/dim[1] # convert y from the higher resolution to the lower resolution images\n",
    "    return x_new,y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having generated the map inside the computer with vision pathplanning is needed to create a path from the starting point to the goal. This is made possible by the pathplanning class.\n",
    "This class is used to create a pathplanning object, which is linked to a specific map, on which then all pathplanning is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise path planner\n",
    "To create the object the constructor of the class needs 3 things, The occupancy grid in form of a numpy array, with which number the occupied cells are marked and many CM represents one pixel(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:52:51.473548Z",
     "start_time": "2020-08-29T12:52:51.462661Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imports needed to run pathplanning\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#import pathplanning class\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from pathPlanning import pathPlaning\n",
    "\n",
    "#load a testing map from image and convert it into a numpy array \n",
    "#(based on: https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays)\n",
    "pil_imgray = Image.open('Images/obstaclesTestMap.jpg').convert('LA')\n",
    "img = np.array(list(pil_imgray.getdata(band=0)), float)\n",
    "img.shape = (pil_imgray.size[1], pil_imgray.size[0])\n",
    "img=img<200\n",
    "occupancyGrid=img.astype(int)\n",
    "plt.imshow(occupancyGrid)\n",
    "##generate pathplanning object for the occupancy grid generated by the testmap\n",
    "pathPlanner=pathPlaning(occupancyGrid.copy(),1,1)#occipied cells are marked with a 1 and 1 cm per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning phase\n",
    "Now that the pathplanning object has a map on which it should plan paths, the goal needs to be set by calling the \"setGoal\" method of the object and hand over a numpy array with the x,y coordinates of the goal.\\[x,y\\]. These coordinates are in cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimer=timer()    #execution time when goal is set once\n",
    "#set goal\n",
    "goal=np.array([90,15])\n",
    "pathPlanner.setGoal(goal)\n",
    "\n",
    "endTimer=timer()\n",
    "print(\"Time needed for planning goal:\",endTimer-startTimer)\n",
    "\n",
    "startTimer=timer()    #execution time when goal is set again\n",
    "#set goal\n",
    "goal=np.array([90,15])\n",
    "pathPlanner.setGoal(goal)\n",
    "\n",
    "endTimer=timer()\n",
    "print(\"Time needed for planning for the same goal again:\",endTimer-startTimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the \"setGoal\" method is called it simultaneously starts the planning phase. This does not mean that a path is planned but that the map is prepared to do fast pathplanning for each starting point one can come up with. This means that if the goal is often changed it will be a lot slower than a A*-pathplanner. If the goal stays the same and just the starting point changes often, it will be quite a bit faster. This is because in this planning phase, which is encapsulated in the private method \\_\\_generateGradient. In it a map is created, where for each grid cell its distance to the goal is calculated. This distance is a path distance, so the value saved means from the current cell you the robot has to move so many cells to reach the goal. This calculation is started from the goal and is made for every cell that is not marked occupied. The result is saved in the private distanceMap attribute, which can be accessed by a getter method \"getDistanceMap\". The occupied cells are left at the starting value of 0. This means during path generation the map needs to be checked whether the cell is occupied or not. But it also means that to make a path the path generator just has to \"roll\" down hill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the distance map visible\n",
    "distanceGrid=pathPlanner.getDistanceMap()\n",
    "fillUp=np.zeros_like(occupancyGrid)#helper to make picture 3 colors\n",
    "maxValue=np.amax(distanceGrid.transpose())   # normalize values to fit into picture\n",
    "#create picture to show  map and distance map\n",
    "pic=np.dstack((occupancyGrid,np.divide(distanceGrid,maxValue),fillUp))\n",
    "plt.imshow(pic)\n",
    "plt.scatter(goal[0],goal[1],marker=\"o\", color = 'yellow',s=200)\n",
    "plt.xlabel('X-direction')\n",
    "plt.ylabel('Y-direction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this picture,in red are the obstacles, in yellow the goal and in green the distance of the pixel to the goal in terms of how much movement would be needed to get to the goal.\n",
    "As can be seen surrounding the goal(the yellow point) are dark areas. This means those cells are very close to the goal. The greener the pixel is the longer the movement required to get to the goal from this pixel(cell). As can be seen if the goal is (90,15)(x,y) in the corner bottom left. There you can see a darker diagonal. This means if the robot is on those cells he needs to move less than if he is positioned at (80,20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Phase\n",
    "Now that the planning phase is over, the pathplanner is ready to provide paths to the goal from every free cell. To start the query phase and actually plan a path the start point needs to be provided. When the start point is provided, the class calls the private method \"\\_\\_generatePath\" to generate the path from the defined start point to the defined goal. For this the method begins at the starting point and looks for the cell, which has the smallest distance to goal of all its neighbors and puts the coordinates of that cell into the path. Of course only cells which are marked as free in the occupancy grid are checked. This goes on until th goal is reached. It is a bi comparable to rolling down a hill, since the cell from where the longest path to the goal exists is at the top of the hill.\n",
    "But this is only possible if \"setGoal\" was called atleast once before! Otherwise it can not find a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-29T12:36:31.127153Z",
     "start_time": "2020-08-29T12:36:25.042891Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "startTimer=timer()\n",
    "start=np.array([10,75])\n",
    "pathPlanner.setStart(start)\n",
    "endTimer=timer()\n",
    "print(\"Time needed for the query for a new path:\",endTimer-startTimer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As may have been noticeable the query phase of the pathplanning is very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the path\n",
    "Now that both planning and query phases are complete, the path can be extracted by calling the method \"getPath\" or \"getOptimizedPath\" The \"getPath\" method returns the path as a numpy array with the first line representing the x-coordinates and the second line representing the y coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unoptimizedPath=pathPlanner.getPath()\n",
    "print(unoptimizedPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"getoptimizedPath\" method returns a path where only the points, where the robot needs to turn, are retained. The path is output in the same format as the path from \"getPath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizedPath=pathPlanner.getOptimizedPath()\n",
    "print(optimizedPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the 2 paths drawn onto the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(occupancyGrid)\n",
    "plt.plot(unoptimizedPath[0], unoptimizedPath[1], marker=\"o\", color = 'blue');\n",
    "plt.scatter(optimizedPath[0],optimizedPath[1], marker=\"o\", color = 'cyan',s=150)\n",
    "plt.xlabel('X-direction')\n",
    "plt.ylabel('Y-direction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cyan dots represent the optimized path and the blue dots represent the unoptimized path. As can be seen for the optimized path only the waypoints where the robot needs to change its orientation, are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complete pathplanning demonstration\n",
    "To see the complete Pathplanning on the testing map inaction this section can be run. To get a explanation of each part, see in the previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports needed to run pathplanning\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#import pathplanning class\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from pathPlanning import pathPlaning\n",
    "\n",
    "#load a testing map from image and convert it into a numpy array \n",
    "#(based on: https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays)\n",
    "pil_imgray = Image.open('Images/obstaclesTestMap.jpg').convert('LA')\n",
    "img = np.array(list(pil_imgray.getdata(band=0)), float)\n",
    "img.shape = (pil_imgray.size[1], pil_imgray.size[0])\n",
    "img=img<200\n",
    "occupancyGrid=img.astype(int)\n",
    "plt.imshow(occupancyGrid)\n",
    "##generate pathplanning object for the occupancy grid generated by the testmap\n",
    "pathPlanner=pathPlaning(occupancyGrid.copy(),1,1)#occipied cells are marked with a 1 and 1 cm per pixel\n",
    "\n",
    "#--set goal\n",
    "startTimer=timer()    #execution time when goal is set once\n",
    "#set goal\n",
    "goal=np.array([90,15])\n",
    "pathPlanner.setGoal(goal)\n",
    "\n",
    "endTimer=timer()\n",
    "print(\"Time needed for planning goal:\",endTimer-startTimer)\n",
    "#---set path\n",
    "startTimer=timer()\n",
    "start=np.array([10,75])\n",
    "pathPlanner.setStart(start)\n",
    "endTimer=timer()\n",
    "print(\"Time needed for the query for a new path:\",endTimer-startTimer)\n",
    "\n",
    "#--- get unoptimized path\n",
    "unoptimizedPath=pathPlanner.getPath()\n",
    "#-- get optimized Path\n",
    "optimizedPath=pathPlanner.getOptimizedPath()\n",
    "#--- draw all the components onto the same picture\n",
    "\n",
    "#make the distance map visible\n",
    "distanceGrid=pathPlanner.getDistanceMap()\n",
    "fillUp=np.zeros_like(occupancyGrid)#helper to make picture 3 colors\n",
    "maxValue=np.amax(distanceGrid.transpose())   # normalize values to fit into picture\n",
    "#create picture to show  map and distance map\n",
    "pic=np.dstack((occupancyGrid,np.divide(distanceGrid,maxValue),fillUp))\n",
    "plt.imshow(pic)\n",
    "plt.scatter(goal[0],goal[1],marker=\"o\", color = 'yellow',s=200)\n",
    "plt.xlabel('X-direction')\n",
    "plt.ylabel('Y-direction')\n",
    "plt.plot(unoptimizedPath[0], unoptimizedPath[1], marker=\"o\", color = 'blue');\n",
    "plt.scatter(optimizedPath[0],optimizedPath[1], marker=\"o\", color = 'cyan',s=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue points represent the unoptimized path, the cyan points are the optimized paths waypoint and red are the occupied cells. The green shade represents the distance of that pixel(cell) to the goal. The greener the longer is the path to the goal from that pixel(cell)\n",
    "As can be seen the path follows closely the obstacle. To prevent the robot from colliding with the obstacles, they need to be enlarged in the occupancy grid before the occupancy grid is given to the constructor. To see different behaviors the goal and start can be changed freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lower = np.array([20,50,90])\n",
    "        upper = np.array([100,100,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "cv2.namedWindow(\"preview\")\n",
    "cameraIndex=1             #TODO: change it to the correct camera. currently uses webcam\n",
    "sat=10\n",
    "exp=-6\n",
    "videoCapture = cv2.VideoCapture(cameraIndex)\n",
    "videoCapture.set(cv2.CAP_PROP_EXPOSURE,-6)\n",
    "videoCapture.set(cv2.CAP_PROP_SATURATION ,10)\n",
    "if not(videoCapture.isOpened()):\n",
    "    raise Exception('could not connect to camera')\n",
    "\n",
    "if videoCapture.isOpened(): # try to get the first frame\n",
    "    rval, frame = videoCapture.read()\n",
    "else:\n",
    "    rval = False\n",
    "    print(\"no image\")\n",
    "\n",
    "while rval:\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "    rval, frame = videoCapture.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27: # exit on ESC\n",
    "        break\n",
    "    elif(key== ord('w')):\n",
    "        exp=exp+0.1\n",
    "        print (videoCapture.set(cv2.CAP_PROP_EXPOSURE,exp),exp)\n",
    "    elif(key== ord('s')):    \n",
    "        exp=exp-0.1\n",
    "        print (videoCapture.set(cv2.CAP_PROP_EXPOSURE,exp),exp)\n",
    "cv2.destroyWindow(\"preview\")\n",
    "cv2.destroyAllWindows() \n",
    "cv2.VideoCapture(cameraIndex).release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "497.208px",
    "left": "281.667px",
    "right": "20px",
    "top": "62px",
    "width": "749px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
